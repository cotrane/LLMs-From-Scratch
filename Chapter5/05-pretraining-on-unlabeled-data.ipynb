{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining on unlabeled data\n",
    "\n",
    "Now that we have defined the GPT model, we can implement a training function which will pre-train the model on unlabeled data.\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "- Compute training and validation set losses\n",
    "- Implement the training function\n",
    "- Pre-Train the model\n",
    "- Save and load the model weights to continue training\n",
    "- Load the original pre-trained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\".\")))\n",
    "from Chapter4.gpt_model import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,  # shorter than the original 1024\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_layers\": 12,\n",
    "    \"n_heads\": 12,\n",
    "    \"drop_rate\": 0.1,  # often set to 0.0\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define helper functions to convert text to tokens and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from Chapter4.generate_text_simple import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text: str, tokenizer: tiktoken.Encoding) -> torch.Tensor:\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    # Add the batch dimension as the first dimension\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids: torch.Tensor, tokenizer: tiktoken.Encoding) -> str:\n",
    "    # Remove the batch dimension here\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=256,\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we haven't yet trained the model, it doesn't do a great job of generating text. In order to see better outcomes, we start by defining the loss metric first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the text generation loss\n",
    "\n",
    "In order to compute the loss of generated text for a given input, we need to go through the following steps we defined in previous chapters:\n",
    "\n",
    "1. Use vocabulary to map the input to token IDs\n",
    "2. Compute the probability row vector for each input token using the softmax function\n",
    "3. Find the index with the highest probability via argmax\n",
    "4. Get all predicted token IDs given by the index positions with the hightest probabilities\n",
    "5. Map the index back to text using the inverse vocabulary\n",
    "\n",
    "In the following we will go through an example with 2 input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257]) # [batch_size, context_length, vocab_size]\n"
     ]
    }
   ],
   "source": [
    "# The input token IDs\n",
    "inputs = torch.tensor([\n",
    "    [16833, 3626, 6100],  # \"Every effort moves\", \n",
    "    [40, 1107, 588]  # \"I really like\"\n",
    "])\n",
    "\n",
    "# The target token IDs\n",
    "targets = torch.tensor([\n",
    "    [3626, 6100, 345],  # \" effort moves you\"\n",
    "    [1107, 588, 11311]  # \" really like chocolate\"\n",
    "])\n",
    "\n",
    "# Feed input token IDs through the model\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape, \"# [batch_size, context_length, vocab_size]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above covers the first two steps in the list. We now need to continue with steps 3-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n",
      "Targets batch 1:  effort moves you\n",
      "Predictions batch 1:  Armed heNetflix\n",
      "Targets batch 2:  really like chocolate\n",
      "Predictions batch 2:  pressuring empoweredfaith\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Predictions batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "print(f\"Targets batch 2: {token_ids_to_text(targets[1], tokenizer)}\")\n",
    "print(f\"Predictions batch 2: {token_ids_to_text(token_ids[1].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training the aim is to increase the probability of the correct target tokens. The untrained model should have random target probabilities which, given the vocabulary size of 50257, should be around $1 / 50257 = 0.00002$\n",
    "\n",
    "We can verify this by printing the probabilities of target tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(f\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(f\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the loss, we need to compute the cross-entropy loss for each input sequence. To do this we will need the logarithm of the probabilities. We then take the mean of those log probabilities and finally multiply them by -1. This is done as we want to *minimize* the loss function. Given the logarithm is increasing, we need to take the negative log to obtain a decreasing function for the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-10.8790, -11.0961, -11.3570,  ..., -10.7061, -11.8728, -10.8829],\n",
      "         [-11.6010, -11.5068, -11.7514,  ..., -10.4451, -12.0220, -11.2076],\n",
      "         [-10.4184, -11.6350, -11.0592,  ..., -10.2472, -11.1697, -11.2109]],\n",
      "\n",
      "        [[-11.2849, -10.7933, -11.1530,  ..., -11.4748, -10.2664, -11.1595],\n",
      "         [-11.8313, -10.9327, -11.4580,  ..., -10.7612, -11.3828, -11.0709],\n",
      "         [-10.4312, -10.3008, -10.1012,  ..., -11.9399,  -9.7516, -11.1983]]])\n",
      "Loss: 10.991869926452637\n",
      "Logits shape:  torch.Size([2, 3, 50257])\n",
      "Targets shape:  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# take the logarithm of the probabilities\n",
    "log_probas = torch.log(probas)\n",
    "print(log_probas)\n",
    "\n",
    "# compute the cross-entropy loss\n",
    "loss = -torch.mean(log_probas)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Logits shape: \", logits.shape)\n",
    "print(\"Targets shape: \", targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function we want to flatten the dimensions of the logits and targets over the batch size. We can do this using the `flatten()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Logits shape:  torch.Size([6, 50257])\n",
      "Flattened Targets shape:  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened Logits shape: \", logits_flat.shape)\n",
    "print(\"Flattened Targets shape: \", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied the softmax, select the largets probability score and compute the negative log probabilities. In practice, we can use the PyTorch function `cross_entropy` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.793964385986328\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate a trained language model, we can look at the *perplexity* of the model. It measures how well the predicted probability distribution matches the distribution of the actual words in the dataset. A lower perplexity is better.\n",
    "\n",
    "It can be computed using $perplexity = torch.exp(loss)$. In our example, this would return $tensor(48725.8203)$ which can be interpreted as the model being unsure which of 48,725 tokens of our vocabulary it should use to generate the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the training and validation set losses\n",
    "\n",
    "Next we will prepare our training and validation datasets and compute the loss for them.\n",
    "\n",
    "We will again use \"The Verdict\" as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 20479\n",
      "Total tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../Chapter2/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "total_characters = len(text)\n",
    "total_tokens = len(tokenizer.encode(text))\n",
    "\n",
    "print(f\"Total characters: {total_characters}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split this dataset into training and validation sets and prepare the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 18431\n",
      "Validation data length: 2048\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * total_characters)\n",
    "\n",
    "train_data = text[:split_idx]\n",
    "val_data = text[split_idx:]\n",
    "\n",
    "print(f\"Train data length: {len(train_data)}\")\n",
    "print(f\"Validation data length: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "0 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "1 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "2 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "3 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "4 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "5 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "6 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "7 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "8 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Validation loader:\n",
      "0 torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "from Chapter2.data_utils import created_dataloader_v1\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = created_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = created_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Iterate over the training and validation batches\n",
    "print(\"Train loader:\")\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    print(i, x.shape, y.shape)\n",
    "\n",
    "print(\"Validation loader:\")\n",
    "for i, (x, y) in enumerate(val_loader):\n",
    "    print(i, x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our small dataset, we only have 9 training batches with 2 samples each and a single validation batch.\n",
    "\n",
    "Next we implement a helper function to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another utility function to compute the loss across all batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, *, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use it to compute the training and validation loss of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 10.987583690219456\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# For Apple Silicon\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device_str = \"mps\"\n",
    "device = torch.device(device_str)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(f\"Train loss: {train_loss}\")\n",
    "print(f\"Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LLM\n",
    "\n",
    "Now that we defined the loss function and a utility function to compute it, we can implement a first version of the training routine, a function to evaluate the model and a generate and print utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, \n",
    "                       device, num_epochs, eval_freq, eval_iter, \n",
    "                       start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            tokens_seen += input_batch.numel()\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss: {train_loss:.3f}, \"\n",
    "                    f\"Val loss: {val_loss:.3f}\"\n",
    "                )\n",
    "                \n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our first GPTModel for 10 epochs using the AdamW optimizer and above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss: 10.506, Val loss: 10.576\n",
      "Ep 1 (Step 000005): Train loss: 9.230, Val loss: 9.416\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 2 (Step 000010): Train loss: 8.648, Val loss: 8.886\n",
      "Ep 2 (Step 000015): Train loss: 8.073, Val loss: 8.426\n",
      "Every effort moves you, the,, the,, the,,,, the, the,,,, the,,,,,,,,, the,, the,,,, the,,, the,,,,,,,,\n",
      "Ep 3 (Step 000020): Train loss: 7.611, Val loss: 7.969\n",
      "Ep 3 (Step 000025): Train loss: 7.170, Val loss: 7.580\n",
      "Every effort moves you, the, the the the the the the the the the the the.                                   \n",
      "Ep 4 (Step 000030): Train loss: 6.644, Val loss: 7.247\n",
      "Ep 4 (Step 000035): Train loss: 6.247, Val loss: 6.976\n",
      "Every effort moves you, and, and the the of the.       \", I had the, the, I had the \".            \", and, and I had the\n",
      "Ep 5 (Step 000040): Train loss: 5.874, Val loss: 6.810\n",
      "Every effort moves you, and, and the of the of the of the of the of the of the, and I had the the, and the of the of the of the the of the, and the the of the of the of the of the of the of\n",
      "Ep 6 (Step 000045): Train loss: 5.398, Val loss: 6.636\n",
      "Ep 6 (Step 000050): Train loss: 5.045, Val loss: 6.520\n",
      "Every effort moves you, and I had the the of the his the, and I had.           \"I. I had been the the my, and I had been, the, and I had been.  \n",
      "Ep 7 (Step 000055): Train loss: 4.730, Val loss: 6.479\n",
      "Ep 7 (Step 000060): Train loss: 4.524, Val loss: 6.366\n",
      "Every effort moves you, and, and in the picture--I to the picture.   \"I was--I, the, I had been to the picture, and I had been, and he was his he had been the picture, and he was his\n",
      "Ep 8 (Step 000065): Train loss: 4.169, Val loss: 6.287\n",
      "Ep 8 (Step 000070): Train loss: 3.769, Val loss: 6.329\n",
      "Every effort moves you know the to the a little of the picture--I had a little of a little to me--I had been, I had to see.                     \n",
      "Ep 9 (Step 000075): Train loss: 3.565, Val loss: 6.262\n",
      "Ep 9 (Step 000080): Train loss: 3.265, Val loss: 6.217\n",
      "Every effort moves you know.      \"--I had a me in a and he was--I--and it's the to see a little of his pictures--I had been his pictures I, a little a--I had been the his\n",
      "Ep 10 (Step 000085): Train loss: 3.005, Val loss: 6.226\n",
      "Every effort moves you know it was not that, one of the to the fact with a little a.        \"Oh, in the; and I had been his pictures--and I, the donkey, and the room, I had\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-1)\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, track_tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, \n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5, \n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss has continued to improve over the full 10 epochs but the validation loss begun to saturate. Let's first plot the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Train loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Creates invisible line to align axis\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, track_tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While both training and validation loss decrease over the first few epochs, they start diverging around epoch 4. This is expected given the small dataset we are using to train and indicates that the model starts to memorize the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the decoding strategy\n",
    "\n",
    "Our current decoder always picks the token with the hightest probability. In order to create some variation in the output we will add two concepts, *temperature_scaling* and *top-k-sampling* to our function.\n",
    "\n",
    "First we print the predicted text of our current implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know it was not that, one of the to the fact with a little a.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Temperature scaling\n",
    "\n",
    " Temperature scaling is a technique used to adjust the probabilities of the model's output. It introduces a temperature parameter that controls the randomness of the predictions.\n",
    "\n",
    " For illustration, we will use a very small vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then hard-code the logits for the next token predicton and convert them to probabilities using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.501, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a probabilistic sampling process, we replace the argmax function with a multinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result in this case is the same because the multinomial function samples the word according to the assinged probabilities. In our manually defined example, the probability for the word \"forward\" is significantly higher than that of any other word.\n",
    "We can implement a function which tracks how often a word will be chosen if we draw 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "344 x toward\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas, num_samples=1000):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for _ in range(num_samples)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it will output \"forward\" in the vast majority of cases. The other possible options also seem to make sense in context of the input sequence \"every effort moves you\" - those are \"toward\", \"closer\" and \"inches\".\n",
    "\n",
    "*Temperature scaling* controls the selection process by chaning the logits so they become more equal. This is achieved by simply dividing them. Let's define a function and use it to compute and plot the sampled frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "\n",
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, t) for t in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(10, 5))\n",
    "for i, t in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f\"T={t}\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that temperatures > 1 smoothen out the distribution and other words become more frequent, while temperatures < 1 emphasize the most likely word even more and suppress other potential words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k sampling\n",
    "\n",
    "While the approach above gave us more diverse output sequences, it also meant that some of them make less sense grammatically. Top-k sampling, combined with probabilistic sampling and temperature scaling can improve this by only allowing samples from the k most likely tokens.\n",
    "\n",
    "We can illustrate the implementation with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5010])\n",
      "Top positions: tensor([3, 7, 0])\n",
      "New logits: tensor([4.5010,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "Top-k probabilities: tensor([0.5779, 0.3612, 0.0610])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)\n",
    "\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(-float(\"inf\")),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(\"New logits:\", new_logits)\n",
    "topk_probas = torch.softmax(top_logits, dim=0)\n",
    "print(\"Top-k probabilities:\", topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with text generation function\n",
    "\n",
    "We now add the functionalities developed above to our text generation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, top_pos = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                condition=logits < min_val,\n",
    "                input=torch.tensor(-float(\"inf\")),\n",
    "                other=logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=-1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" she to. \" was one of that, my by his\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    temperature=1.4,\n",
    "    top_k=25\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we comment out the manual seed and generate outputs multiple times, we will see that the output varies significantly. How much it varies can be controlled by the new parameters `temperature` and `top_k`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and saving weights\n",
    "\n",
    "So far we have randomly initialised the weights and trained a model from scratch. In practice, training a full-scale LLM is very expensive and we usually start from pre-trained weights. \n",
    "\n",
    "In this section we will discuss how to save and load model weights.\n",
    "\n",
    "The recommended way of saving a trained PyTorch model is shown below. It will create a file with the state_dict containing a dictionary of each layer with it's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subsequently load the saved weight again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a model in this way is good if we want to do inference with it. If we are planning on continuing to train, starting from the saved state, we need to also save the state of the optimizer with it. Otherwise it will initialize without its parameters and training might not converge properly. This will then allow us to load the full state of the training process and continue fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \"model_and_optimizer.pth\")\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pretrained weights from OpenAI\n",
    "\n",
    "We will now load the weights from OpenAI's GPT-2 model. First, we have to download the weights for which we will use a script from the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x390d92610>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-From-Scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split(\"/\")[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:00<00:00, 25.0kiB/s]\n",
      "encoder.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 1.27MiB/s]\n",
      "hparams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.0/90.0 [00:00<00:00, 32.5kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498M/498M [03:04<00:00, 2.69MiB/s] \n",
      "model.ckpt.index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.21k/5.21k [00:00<00:00, 1.74MiB/s]\n",
      "model.ckpt.meta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471k/471k [00:00<00:00, 819kiB/s] \n",
      "vocab.bpe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 787kiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "from Chapter5.gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\",\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have downloaded the weights for the smallest GPT-2 model with 124M parameters. Other, larger, versions are available and can be downloaded in the same way. Let's create a dictionary with the different available GPT versions first and update the definition of the smallest model from earlier and initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_heads\": 12, \"n_layers\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_heads\": 16, \"n_layers\": 24},\n",
    "    \"gpt2-large (760M)\": {\"emb_dim\": 1280, \"n_heads\": 20, \"n_layers\": 36},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_heads\": 25, \"n_layers\": 48},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign the pre-trained weights to our model, we first define a small function which checks that the initialized tensor and the weight have the same shape. If it doesn't, it will raise a ValueError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {left.shape} != {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.W_query.weight = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_query.weight, q_w.T\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.W_key.weight = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_key.weight, k_w.T\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.W_value.weight = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_value.weight, v_w.T\n",
    "        )\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.W_query.bias = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_query.bias, q_b\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.W_key.bias = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_key.bias, k_b\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.W_value.bias = assign(\n",
    "            gpt.transformer_blocks[b].attention.W_value.bias, v_b\n",
    "        )\n",
    "\n",
    "        gpt.transformer_blocks[b].attention.out_proj.weight = assign(\n",
    "            gpt.transformer_blocks[b].attention.out_proj.weight,\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_proj\"])[\"w\"].T,\n",
    "        )\n",
    "        gpt.transformer_blocks[b].attention.out_proj.bias = assign(\n",
    "            gpt.transformer_blocks[b].attention.out_proj.bias,\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_proj\"])[\"b\"],\n",
    "        )\n",
    "\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[0].weight = assign(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[0].weight,\n",
    "            (params[\"blocks\"][b][\"mlp\"][\"c_fc\"])[\"w\"].T,\n",
    "        )\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[0].bias = assign(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[0].bias,\n",
    "            (params[\"blocks\"][b][\"mlp\"][\"c_fc\"])[\"b\"],\n",
    "        )\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[2].weight = assign(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[2].weight,\n",
    "            (params[\"blocks\"][b][\"mlp\"][\"c_proj\"])[\"w\"].T,\n",
    "        )\n",
    "        gpt.transformer_blocks[b].feed_forward.layers[2].bias = assign(\n",
    "            gpt.transformer_blocks[b].feed_forward.layers[2].bias,\n",
    "            (params[\"blocks\"][b][\"mlp\"][\"c_proj\"])[\"b\"],\n",
    "        )\n",
    "\n",
    "        gpt.transformer_blocks[b].ln1.scale = assign(\n",
    "            gpt.transformer_blocks[b].ln1.scale,\n",
    "            (params[\"blocks\"][b][\"ln_1\"])[\"g\"],\n",
    "        )\n",
    "        gpt.transformer_blocks[b].ln1.shift = assign(\n",
    "            gpt.transformer_blocks[b].ln1.shift,\n",
    "            (params[\"blocks\"][b][\"ln_1\"])[\"b\"]\n",
    "        )\n",
    "        gpt.transformer_blocks[b].ln2.scale = assign(\n",
    "            gpt.transformer_blocks[b].ln2.scale,\n",
    "            (params[\"blocks\"][b][\"ln_2\"])[\"g\"]\n",
    "        )\n",
    "        gpt.transformer_blocks[b].ln2.shift = assign(\n",
    "            gpt.transformer_blocks[b].ln2.shift,\n",
    "            (params[\"blocks\"][b][\"ln_2\"])[\"b\"]\n",
    "        )\n",
    "\n",
    "    gpt.final_norm.scale = assign(\n",
    "        gpt.final_norm.scale, params[\"g\"]\n",
    "    )\n",
    "    gpt.final_norm.shift = assign(\n",
    "        gpt.final_norm.shift, params[\"b\"]\n",
    "    )\n",
    "    # Out head reuses the weights from the token embedding layer\n",
    "    gpt.out_head.weight = assign(\n",
    "        gpt.out_head.weight, params[\"wte\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the model with pre-trained weights to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward finding an ideal new way to practice something!\n",
      "\n",
      "What makes us want to be on top of that?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
