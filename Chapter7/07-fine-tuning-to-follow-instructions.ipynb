{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning to follow instructions\n",
    "\n",
    "In order to follow instructions properly, the model pre-trained on text completion requires another stage of supervised instruction fine-tuning. Here, we use a dataset of instructions and it's desired answers to make the model better respond to such inputs. One of the most important steps here is the creation of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "We start by downloading an instruction dataset with 1100 instruction-response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the dataset: 1100\n",
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n",
      "Another example:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "\n",
    "def download_and_load_file(file_path: str, url: str):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            text_data = f.read()\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries in the dataset:\", len(data))\n",
    "\n",
    "print(\"Example entry:\\n\", data[50])\n",
    "print(\"Another example:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the examples above we can see that each sample is a dictionary with keys `instruction`, `input` and `output`. Field `input` might be empty for some cases.\n",
    "\n",
    "There are different ways of formatting those inputs for instruction fine-tuning. We will use Alpaca-style formatting throughout this notebook. It encodes the inputs and outputs as follows:\n",
    "\n",
    "```text\n",
    "\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instructions:\n",
    "Identify the correct spelling of the following word.\n",
    "\n",
    "### Input:\n",
    "Ocassion\n",
    "\n",
    "### Response:\n",
    "The correct spelling is 'Occasion'.\n",
    "```\n",
    "\n",
    "We will define a function called `format_input` to convert the json samples into the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instructions:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "def format_input(entry: dict[str, str]) -> str:\n",
    "    instruction_text = (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instructions:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "\n",
    "    return instruction_text + input_text\n",
    "\n",
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example without an input value, this part will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instructions:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now divide the dataset into train, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing data into training batches\n",
    "\n",
    "As the next step, we need to make sure that the model receives the formatted data in the required batches. We will use a PyTorch dataloader for this as in the previous chapters, but write a custom collate function for it which will define the steps to create the required input.\n",
    "\n",
    "As a first step we will define a `InstructionDataset` class which applies the format_input and *pretokenizes* the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data: list[dict[str, str]], tokenizer: Any):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index: int) -> list[int]:\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to classification fine-tuning, we need to use a special padding token to indicate the end of a sample text. We will again use `<|endoftext|>` for this purpose and append the corresponding token ID to the encoded text directly rather than append the string to the text. The token ID for this padding string is `50256`.\n",
    "\n",
    "We implement the padding process in the custom collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch: list[list[int]], pad_token_id: int = 50256, device=\"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "    inputs_lst = []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collate function can be integrated into a `DataLoader` as well as serve as a standalone function. To test it, we will use it as a separate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = [\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3,\n",
    "]\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our collate function correctly pads the various input sample to the longest sample in the batch.\n",
    "\n",
    "We will now update the collate function to also generate the target token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch: list[list[int]], pad_token_id: int = 50256, device=\"cpu\"\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        targets_lst.append(targets)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we will replace the padding tokens after the first one with the special value `-100`. This will exclude those tokens from the loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch: list[list[int]], \n",
    "    pad_token_id: int = 50256, \n",
    "    ignore_index: int = -100, \n",
    "    allowed_max_length: int | None = None, \n",
    "    device: torch.device = torch.device(\"cpu\")\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "    if allowed_max_length is not None:\n",
    "        batch_max_length = min(batch_max_length, allowed_max_length)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        \n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "            \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "        \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target tokens are now changed such that all padding tokens after the first one are exchanged to the ignore_token.\n",
    "\n",
    "We can see the impact of this on a loss function in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "tensor(0.7936)\n",
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1]) \n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)\n",
    "\n",
    "# Adding another token ID\n",
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)\n",
    "\n",
    "# Replacing the extra token ID with -100\n",
    "logits_3 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_3, targets_3)\n",
    "print(loss_3)\n",
    "\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason the loss function ignores values `-100` is that function `cross_entropy` by default has argument `ignore_index=-100`.\n",
    "\n",
    "Often, not only padding tokens are masked, but also the instruction itself. In this way, only the generated answer will be evaluated through the loss and the instruction is ignored. This can reduce overfitting during training.\n",
    "\n",
    "In the following we will not mask the instructions here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data loaders\n",
    "\n",
    "With the dataset and collate function defined, we can now define the dataloaders we will use to shuffle and batch the input dataset.\n",
    "\n",
    "We start by setting the device variable as well as the allowed_max_length variable and adding it to the collate function definition through the `functools.partial` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    print(inputs.shape, targets.shape)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the input shapes we can see that each batch has a different token length, depending on the samples which are part of the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pretrained LLM\n",
    "\n",
    "Now that we have defined dataset and data loaders for instruction fine-tuning, we need to load a pretrained LLM that we want to fine-tune. We will use the code written in the previous chapter to load GPT-2 and pick the medium sized version with 355M parameters for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\".\")))\n",
    "\n",
    "from Chapter4.gpt_model import GPTModel\n",
    "from Chapter5.gpt_download import download_and_load_gpt2\n",
    "from Chapter5.load_weights_for_gpt import load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start fine-tuning, let's look at the performance of the pretrained model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instructions:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chapter5.eval_utils import generate\n",
    "from Chapter5.tokenize_utils import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `generate` returns the combined input and output text. This made sense for text completion models but is not desired here where we want to evaluate the model only on the response. For this reason we will strip away the input text from the `generated_text` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Example:\n",
      "\n",
      "### Example:\n",
      "\n",
      "### Example:\n",
      "\n",
      "### Example:\n"
     ]
    }
   ],
   "source": [
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows us that the pretrained model struggles to generate a good response for this task. We will now implement the fine-tuning to improve the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the LLM on instruction data\n",
    "\n",
    "We will re-use the loss calculation function from the previous chapters and evaluate the initial loss for the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8545480728149415\n",
      "Validation loss: 3.7824447631835936\n"
     ]
    }
   ],
   "source": [
    "from Chapter5.eval_utils import train_model_simple\n",
    "from Chapter5.loss_utils import calc_loss_loader\n",
    "\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to fine-tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss: 2.737, Val loss: 2.709\n",
      "Ep 1 (Step 000005): Train loss: 1.213, Val loss: 1.129\n",
      "Ep 1 (Step 000010): Train loss: 0.881, Val loss: 0.958\n",
      "Ep 1 (Step 000015): Train loss: 0.862, Val loss: 0.910\n",
      "Ep 1 (Step 000020): Train loss: 0.784, Val loss: 0.889\n",
      "Ep 1 (Step 000025): Train loss: 0.763, Val loss: 0.858\n",
      "Ep 1 (Step 000030): Train loss: 0.800, Val loss: 0.844\n",
      "Ep 1 (Step 000035): Train loss: 0.721, Val loss: 0.811\n",
      "Ep 1 (Step 000040): Train loss: 0.676, Val loss: 0.802\n",
      "Ep 1 (Step 000045): Train loss: 0.635, Val loss: 0.792\n",
      "Ep 1 (Step 000050): Train loss: 0.671, Val loss: 0.785\n",
      "Ep 1 (Step 000055): Train loss: 0.764, Val loss: 0.777\n",
      "Ep 1 (Step 000060): Train loss: 0.725, Val loss: 0.756\n",
      "Ep 1 (Step 000065): Train loss: 0.653, Val loss: 0.742\n",
      "Ep 1 (Step 000070): Train loss: 0.539, Val loss: 0.736\n",
      "Ep 1 (Step 000075): Train loss: 0.569, Val loss: 0.734\n",
      "Ep 1 (Step 000080): Train loss: 0.604, Val loss: 0.726\n",
      "Ep 1 (Step 000085): Train loss: 0.511, Val loss: 0.707\n",
      "Ep 1 (Step 000090): Train loss: 0.564, Val loss: 0.692\n",
      "Ep 1 (Step 000095): Train loss: 0.505, Val loss: 0.684\n",
      "Ep 1 (Step 000100): Train loss: 0.502, Val loss: 0.677\n",
      "Ep 1 (Step 000105): Train loss: 0.564, Val loss: 0.671\n",
      "Ep 1 (Step 000110): Train loss: 0.553, Val loss: 0.666\n",
      "Ep 1 (Step 000115): Train loss: 0.508, Val loss: 0.662\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instructions: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instructions: Convert the active sentence to passive: 'The\n",
      "Ep 2 (Step 000120): Train loss: 0.435, Val loss: 0.669\n",
      "Ep 2 (Step 000125): Train loss: 0.449, Val loss: 0.685\n",
      "Ep 2 (Step 000130): Train loss: 0.448, Val loss: 0.684\n",
      "Ep 2 (Step 000135): Train loss: 0.407, Val loss: 0.684\n",
      "Ep 2 (Step 000140): Train loss: 0.413, Val loss: 0.683\n",
      "Ep 2 (Step 000145): Train loss: 0.374, Val loss: 0.683\n",
      "Ep 2 (Step 000150): Train loss: 0.379, Val loss: 0.676\n",
      "Ep 2 (Step 000155): Train loss: 0.416, Val loss: 0.681\n",
      "Ep 2 (Step 000160): Train loss: 0.414, Val loss: 0.690\n",
      "Ep 2 (Step 000165): Train loss: 0.380, Val loss: 0.693\n",
      "Ep 2 (Step 000170): Train loss: 0.324, Val loss: 0.690\n",
      "Ep 2 (Step 000175): Train loss: 0.339, Val loss: 0.677\n",
      "Ep 2 (Step 000180): Train loss: 0.390, Val loss: 0.662\n",
      "Ep 2 (Step 000185): Train loss: 0.415, Val loss: 0.660\n",
      "Ep 2 (Step 000190): Train loss: 0.338, Val loss: 0.649\n",
      "Ep 2 (Step 000195): Train loss: 0.330, Val loss: 0.634\n",
      "Ep 2 (Step 000200): Train loss: 0.308, Val loss: 0.635\n",
      "Ep 2 (Step 000205): Train loss: 0.347, Val loss: 0.631\n",
      "Ep 2 (Step 000210): Train loss: 0.365, Val loss: 0.632\n",
      "Ep 2 (Step 000215): Train loss: 0.400, Val loss: 0.635\n",
      "Ep 2 (Step 000220): Train loss: 0.302, Val loss: 0.644\n",
      "Ep 2 (Step 000225): Train loss: 0.349, Val loss: 0.657\n",
      "Ep 2 (Step 000230): Train loss: 0.291, Val loss: 0.653\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instructions: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked everyday by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instructions: What is the capital of the United Kingdom\n",
      "Total training time: 23.11 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, \n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5, \n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Total training time: {execution_time_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss has reduced significantly for both training and validation set. The sample outputs also indicate that the responses are better executing the specified task.\n",
    "\n",
    "Let's plot the loss curve to better see the learning process during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHpCAYAAACful8UAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAf4pJREFUeJzt3Qd4VFX+//FPegMCCRB67yBVQECxgCL27rr2tfzsdVfXv11X3bWvrt217KprB3tBFFFEUXov0luAhJBG+vyf75nMZIIEQwlT8n49z/XO3GknySXmc8853xPl8Xg8AgAAAAAA+1z0vn9LAAAAAABgCN0AAAAAANQRQjcAAAAAAHWE0A0AAAAAQB0hdAMAAAAAUEcI3QAAAAAA1BFCNwAAAAAAdYTQDQAAAABAHSF0AwAAAABQRwjdAACEkZUrVyoqKkqzZs0KdlMAAEAtELoBANjPLDTvarvrrruC3UQAALCPxO6rNwIAALWzYcMG/+233npLd9xxhxYvXuw/1qBBgyC1DAAA7Gv0dAMAsJ+1aNHCv6Wmprrebd/95s2b69FHH1WbNm2UkJCg/v376/PPP6/xvcrLy/WnP/1JPXr00OrVq92xDz74QAMHDlRiYqI6deqku+++W2VlZf7X2Oe9+OKLOvnkk5WcnKyuXbvqww8/9D++detWnX322WrWrJmSkpLc4y+//HKNbXj33Xd1wAEHuOemp6dr9OjRKigo8D9un9WzZ0/XHmvn008/Xe31a9as0RlnnKHGjRsrLS1NJ554ohtG73PBBRfopJNO0sMPP6yWLVu6z7jyyitVWlq6B999AAD2L0I3AAAh5J///KceeeQRFzDnzJmjMWPG6IQTTtDSpUt/89zi4mKdfvrpbn73d999p3bt2rn9eeedp2uvvVYLFizQc889p1deeUX33XdftddaELega59xzDHHuJCdnZ3tHrv99tvdaz/77DMtXLhQzzzzjJo2bVpjr/1ZZ53lgr89d9KkSTrllFPk8Xjc46+//rrrybfPt8fvv/9+9/6vvvqqe9yCs32NDRs2dG2fMmWK6+k/+uijVVJS4v+cb775Rr/++qvb22vta7INAICQ5wEAAEHz8ssve1JTU/33W7Vq5bnvvvuqPWfw4MGeK664wt1esWKFpVnPd9995xk1apTn4IMP9uTk5Pifa8fuv//+aq//73//62nZsqX/vr3+tttu89/Pz893xz777DN3//jjj/dceOGFtWr/9OnT3WtXrly508c7d+7seeONN6odu/feez3Dhg3zt6179+6eiooK/+PFxcWepKQkzxdffOHun3/++Z727dt7ysrK/M85/fTTPWeeeWat2ggAQDAxpxsAgBCRm5ur9evXa8SIEdWO2/3Zs2dXO2a9yzYE/euvv3bDun3sedZbHNizbUPQi4qKVFhY6IaTm759+/ofT0lJUaNGjbRp0yZ3//LLL9epp56qGTNm6KijjnJDu4cPH77TNvfr10+jRo1yw8utx9qef9ppp6lJkyZuiLn1Tl900UW65JJL/K+xoe42rN7X3mXLlrme7kDWXnutT+/evRUTE+O/b8PM586dW+vvLQAAwULoBgAgDNmQ8Ndee01Tp07VEUcc4T+en5/vho7bEO8d2Zxqn7i4uGqP2TzviooKd3vs2LFatWqVPv30U02YMMGFaptDbUPed2RB2J7zww8/6Msvv9STTz6pW2+9VT/99JM/4L/wwgsaOnTob17na++gQYPcMPQd2Zzy2rQXAIBQRugGACBEWG9zq1atXE/1oYce6j9u94cMGVLtudYb3adPHzff+5NPPvE/3wqoWSX0Ll267FVbLPCef/75bjvkkEP0l7/8Zaeh2xeArTfeNpu/3b59e40bN0433HCD+3qWL1/u5ozvjLXXKrhbATn7+gEAiDSEbgAAQoiF2zvvvFOdO3d2lcutargVSttZT/DVV1/tho4fd9xxrujZwQcf7EKv3beiajbMOzo62g3hnjdvnv72t7/Vqg32Htb7bEO6rVjbxx9/7KqP74z1aE+cONENK7fgbPc3b97sf771ul9zzTVuOLkVR7P3++WXX1yFdAvlFsYfeughV7H8nnvucUPmrZf9/fff10033eTuAwAQzgjdAACEEAuo27Zt04033ujmWPfq1cst52XLdu3Mdddd54ZZ23BzW1rM5lVbSLYA+49//MMNy7Zlui6++OJatyE+Pl633HKLW7bL5otbT/ebb7650+da7/TkyZP1+OOPuznp1stt1ddtiLqxz7Vh5has7YKCzR+3+d/WbmOP2etvvvlmNyQ+Ly9PrVu3dkPa6fkGAESCKKumFuxGAAAAAAAQiVinGwAAAACAOkLoBgAAAACgjhC6AQAAAACoI4RuAAAAAADqCKEbAAAAAIA6QugGAAAAAKCOELpDxFNPPaUOHTooMTFRQ4cO1bRp04LdJNTSAw88oMGDB6thw4Zq3ry5TjrpJC1evLjac4qKinTllVcqPT1dDRo00KmnnqrMzMxqz1m9erWOPfZYt2atvY+tZ1tWVlbtOZMmTdLAgQOVkJCgLl266JVXXvlNeziXQsPf//53RUVF+dciNpwH9ce6det0zjnnuJ+1rXNt61L/8ssv/sdttc477rhDLVu2dI+PHj1aS5curfYe2dnZOvvss91a1Y0bN9ZFF12k/Pz8as+ZM2eOW0Pbfs5t27bVgw8++Ju2vPPOO26dbnuOtePTTz+tw68cPuXl5br99tvVsWNH9zPu3Lmz7r33Xvez9+E8iDy25vzxxx+vVq1auf8HjB8/vtrjofQzr01bsO/Pg9LSUt18883uZ5KSkuKec95552n9+vXV3oPzIMLYOt0IrjfffNMTHx/veemllzzz58/3XHLJJZ7GjRt7MjMzg9001MKYMWM8L7/8smfevHmeWbNmeY455hhPu3btPPn5+f7nXHbZZZ62bdt6Jk6c6Pnll188Bx10kGf48OH+x8vKyjx9+vTxjB492jNz5kzPp59+6mnatKnnlltu8T9n+fLlnuTkZM8NN9zgWbBggefJJ5/0xMTEeD7//HP/cziXQsO0adM8HTp08PTt29dz7bXX+o9zHtQP2dnZnvbt23suuOACz08//eR+Zl988YVn2bJl/uf8/e9/96SmpnrGjx/vmT17tueEE07wdOzY0bN9+3b/c44++mhPv379PD/++KPnu+++83Tp0sVz1lln+R/ftm2bJyMjw3P22We73z//+9//PElJSZ7nnnvO/5wpU6a48+PBBx9058ttt93miYuL88ydO3c/fkfqp/vuu8+Tnp7u+fjjjz0rVqzwvPPOO54GDRp4/vnPf/qfw3kQeez39q233up5//337eqKZ9y4cdUeD6WfeW3agn1/HuTk5Lj/z7/11lueRYsWeaZOneoZMmSIZ9CgQdXeg/MgshC6Q4D9Q7vyyiv998vLyz2tWrXyPPDAA0FtF/bMpk2b3C/Yb7/91v/L1X7B2R9cPgsXLnTPsV+0vl/O0dHRno0bN/qf88wzz3gaNWrkKS4udvdvuukmT+/evat91plnnulCvw/nUvDl5eV5unbt6pkwYYLn0EMP9YduzoP64+abb/YcfPDBNT5eUVHhadGiheehhx7yH7PzIyEhwf3RZOyPIzs3fv75Z/9zPvvsM09UVJRn3bp17v7TTz/tadKkif/c8H129+7d/ffPOOMMz7HHHlvt84cOHer5v//7v3301aIm9n3/05/+VO3YKaec4v5ANpwHkW/HsBVKP/PatAX7xs4uvuzsYr09b9WqVe4+50HkYXh5kJWUlGj69OluKIdPdHS0uz916tSgtg17Ztu2bW6flpbm9vbztaFEgT9jG+bTrl07/8/Y9jbkJyMjw/+cMWPGKDc3V/Pnz/c/J/A9fM/xvQfnUmiw4eM2PHzHnxXnQf3x4Ycf6sADD9Tpp5/upggMGDBAL7zwgv/xFStWaOPGjdV+RqmpqW4aQOC5YMMJ7X187Pn2s/zpp5/8zxk5cqTi4+OrnQs2vWXr1q21Ol9Qd4YPH66JEydqyZIl7v7s2bP1/fffa+zYse4+50H9E0o/89q0Bfv3b0cbhm4/e8N5EHkI3UG2ZcsWN+8r8I9sY/ftHwHCS0VFhZvDO2LECPXp08cds5+j/UL0/SLd2c/Y9js7B3yP7eo5Fsi2b9/OuRQC3nzzTc2YMcPN898R50H9sXz5cj3zzDPq2rWrvvjiC11++eW65ppr9Oqrr7rHfT+HXf2MbG+BPVBsbKy7mLcvzhfOhbr317/+VX/4wx/cxbW4uDh38cX+/2BzNA3nQf0TSj/z2rQF+4fVe7E53meddZabv204DyJPbLAbAERaL+e8efNcbwbqlzVr1ujaa6/VhAkTXLES1O+Lb9Y7cf/997v7Frbs98Kzzz6r888/P9jNw37y9ttv6/XXX9cbb7yh3r17a9asWS50W9EkzgMAxkbAnXHGGa6YmV2sReSipzvImjZtqpiYmN9UMLb7LVq0CFq7sPuuuuoqffzxx/rmm2/Upk0b/3H7OdqQ35ycnBp/xrbf2Tnge2xXz7GrolZtknMpuGxI96ZNm1xVcbsabdu3336rJ554wt22q8acB/WDVYDt1atXtWM9e/Z0lemN7+ewq5+R7e18CmRV7K2a7b44XzgX6p6tPODr7bZpI+eee66uv/56/0gYzoP6J5R+5rVpC/ZP4F61apW7YO/r5TacB5GH0B1kNtx00KBBbt5XYC+J3R82bFhQ24basauTFrjHjRunr7/+2i0PE8h+vja0MPBnbPNt7A9w38/Y9nPnzq32C9b3C9j3x7s9J/A9fM/xvQfnUnCNGjXK/QytN8u3WW+nDSX13eY8qB9sesmOywbavN727du72/Y7wv6YCfwZ2fQAm6cXeC7YBRq7mONjv1/sZ2lz7XzPsWVp7A+3wHOhe/fuatKkSa3OF9SdwsJCN/8ykF0Qs5+h4Tyof0LpZ16btqDuA7ctzfXVV1+55SUDcR5EoGBXcoN3eR+rEvjKK6+4aoWXXnqpW94nsIIxQtfll1/ullqYNGmSZ8OGDf6tsLCw2lJRtozY119/7ZaKGjZsmNt2XCrqqKOOcsuO2fJPzZo12+lSUX/5y19c1eunnnpqp0tFcS6FjsDq5YbzoH6wKrSxsbFuyailS5d6Xn/9dfcze+2116ot0WI/kw8++MAzZ84cz4knnrjTZYMGDBjglh37/vvvXVX8wOVirMKsLRdz7rnnuuVi7Odun7PjcjHWlocfftidL3feeSdLRe0n559/vqd169b+JcNs6SBbAtBWIPDhPIjMFSxsyUfb7M/sRx991N32VaUOpZ95bdqCfX8elJSUuGW52rRp4/5fH/i3Y2Alcs6DyELoDhG21q79MW5r69pyP7YmH8KD/TLd2WZrd/vYL64rrrjCLe1gvxBPPvlk98s10MqVKz1jx451ayzaH2Y33nijp7S0tNpzvvnmG0///v3dedKpU6dqn+HDuRS6oZvzoP746KOP3AUUu/jRo0cPz/PPP1/tcVum5fbbb3d/MNlzRo0a5Vm8eHG152RlZbk/sGxtZ1s27sILL3R/yAWyNVVteTJ7Dwt49sfTjt5++21Pt27d3Llgy8198skndfRVI1Bubq7792//DhMTE92/VVu3N/CPas6DyGO/n3f2N4FdhAm1n3lt2oJ9fx7YRbia/na01/lwHkSWKPtPsHvbAQAAAACIRMzpBgAAAACgjhC6AQAAAACoI4RuAAAAAADqCKEbAAAAAIA6QugGAAAAAKCOELpDSHFxse666y63R/3FeQDDeQDDeQAfzgUYzgMYzoPww5JhISQ3N1epqanatm2bGjVqFOzmIEg4D2A4D2A4D+DDuQDDeQDDeRB+6OkGAAAAAKCOELoBAAAAAKgjsapnysrKNHPmTGVkZCg6OrSuOeTl5bn9unXr3LAR1E+cBzCcBzCcB/DhXIDhPIDhPAgdFRUVyszM1IABAxQbW3O0rndzun/++WcNGTIk2M0AAAAAAESAadOmafDgwTU+Xu96uq2H2/eNadmyZbCbAwAAAAAIQxs2bHAdur6MWZN6F7p9Q8otcLdp0ybYzQEAAAAAhLHfm7YcWpOaAQAAAACIIIRuAAAAAADqCKEbAAAAAIA6Uu/mdAMAAACIbOXl5SotLQ12MxDm4uLiFBMTs9fvQ+gGAAAAEBFsNeSNGzcqJycn2E1BhGjcuLFatGihqKioPX4PQjcAAACAiOAL3M2bN1dycvJeBSXUbx6PR4WFhdq0aZO7vzfLTRO6AQAAAETEkHJf4E5PTw92cxABkpKS3N6Ct51XezrUnEJqAAAAAMKebw639XAD+4rvfNqbGgGEbgAAAAARgyHlCLXzidANAAAAAEAdIXQDAAAAAFBHCN0AAAAAEGE6dOigxx9/POjvAUI3AAAAAAR1zvCutrvuumuP3vfnn3/WpZdeus/bi93HkmEhqLzCoxVb8pWVX6IhHdMoBgEAAABEqA0bNvhvv/XWW7rjjju0ePFi/7EGDRpUWzvalkaLjf39GNesWbM6aC32BD3dIaikrEKjH52sM5//UQUl5cFuDgAAABCWLKQWlpQFZbPPro0WLVr4t9TUVNfh5ru/aNEiNWzYUJ999pkGDRqkhIQEff/99/r111914oknKiMjw4XywYMH66uvvtrl0HB73xdffFEnn3yyWwara9eu+vDDD3fr+7l69Wr3ufaZjRo10hlnnKHMzEz/47Nnz9bhhx/u2myPW5t/+eUX99iqVat0/PHHq0mTJkpJSVHv3r316aefqj6gpzsEJcXHKCkuRttLy5WVX6wGCfyYAAAAgN1lf0/3uuOLoHz2gnvGKDl+3/wd/9e//lUPP/ywOnXq5ELrmjVrdMwxx+i+++5zQfw///mPC7TWQ96uXbsa3+fuu+/Wgw8+qIceekhPPvmkzj77bBeG09LSfrcNFRUV/sD97bffqqysTFdeeaXOPPNMTZo0yT3H3m/AgAF65plnFBMTo1mzZikuLs49Zs8tKSnR5MmTXehesGBBtV78SEaaC1HpDeK1dut2ZRWUqH16SrCbAwAAACBI7rnnHh155JH++xaS+/Xr579/7733aty4ca7n+qqrrqrxfS644AKdddZZ7vb999+vJ554QtOmTdPRRx/9u22YOHGi5s6dqxUrVqht27bumIV967G2+ePW22494X/5y1/Uo0cP97j1pvvYY6eeeqoOOOAAd98uINQXhO4QlZ7iDd3Z+SXBbgoAAAAQlmz0qPU4B+uz95UDDzyw2v38/HxXYO2TTz5xc8Kt13n79u0u2O5K3759/bett9mGgG/atKlWbVi4cKEL277AbXr16qXGjRu7xyx033DDDbr44ov13//+V6NHj9bpp5+uzp07u+dec801uvzyy/Xll1+6xyyAB7YnkjGnO0S1TyxUl6i12ppXEOymAAAAAGHJ5jHbEO9gbPuyGLIF5EB//vOfXc+29VZ/9913bhi39SDb8O1d8Q31Dvz+2LDxfcUuBMyfP1/HHnusvv76axfKx40b5x6zML58+XKde+65rsfcLiTYEPf6gNAdoh5ed46+SrhJxdm7vloFAAAAoH6ZMmWKGypuRdEsbFvRtZUrV9bpZ/bs2dPNJbfNx+Zl5+TkuHDt061bN11//fWuR/uUU07Ryy+/7H/Meskvu+wyvf/++7rxxhv1wgsvqD4gdIeo7XFN3L5k2+ZgNwUAAABACLG50hZcrYfbKob/8Y9/3Kc91jtjQ8It4FuxtBkzZri54Oedd54OPfRQ12ttw9ttPrkVVbPibHZhwOZ69+zZ073+uuuu0xdffOHmhNvrv/nmG/9jkY7QHaJK4hu7fXn+lmA3BQAAAEAIefTRR10V8+HDh7uq5WPGjNHAgQPr9DNtKPoHH3zgPnfkyJEuhFsxNFtb3Fi18qysLBfErbfblhMbO3asq5hubH1xq2BuQdsKt9lznn76adUHUZ7aLiAXIdauXeuGNdiwiDZt2ihUbXzqGLXYPEUvpP1Zl1xze7CbAwAAAIS0oqIi14vasWNHJSYmBrs5qAfn1dpaZkt6ukNVcrrbRRdlB7slAAAAAIA9ROgOUdENmrl9QjGhGwAAAADCFaE7RMU3aur2iaU5qmczAAAAAAAgYhC6Q1RS4wy3T/XkqbCkPNjNAQAAAADsAUJ3iEpo6B1enhaVq6z8XS9yDwAAAAAITYTuUJXiHV7eRHnKKigOdmsAAAAAAHuA0B3i1cvTo3KVXUBPNwAAAACEI0J3iIfu1KhCZecVBLs1AAAAAIA9QOgOVUlNVKEod7MwZ3OwWwMAAAAA2AOE7lAVHaNXev1bI4sf07qSpGC3BgAAAEAIO+yww3Tdddf573fo0EGPP/74Ll8TFRWl8ePH7/Vn76v32ZW77rpL/fv3VzgidIewkoz+Wu3J0JbCimA3BQAAAEAdOP7443X00Ufv9LHvvvvOBdo5c+bs9vv+/PPPuvTSS7U/gu+GDRs0duzYffpZkYTQHcLSUuLdnkJqAAAAQGS66KKLNGHCBK1du/Y3j7388ss68MAD1bdv391+32bNmik5OVn7Q4sWLZSQkLBfPiscEbpDWPec7/WX2DfVausvwW4KAAAAEL5KCnZ/Ky+rer3dtmOl22v3vrvhuOOOcwH5lVdeqXY8Pz9f77zzjgvlWVlZOuuss9S6dWsXpA844AD973//2+X77ji8fOnSpRo5cqQSExPVq1cvF/R3dPPNN6tbt27uMzp16qTbb79dpaWl7jFr3913363Zs2e73nfbfG3ecXj53LlzdcQRRygpKUnp6emux92+Hp8LLrhAJ510kh5++GG1bNnSPefKK6/0f1ZtVFRU6J577lGbNm1c4Lce+M8//9z/eElJia666ir3/vY1t2/fXg888IB7zOPxuF77du3aude2atVK11xzjepKbJ29M/Za603f6srYD/Vi4f65QgUAAABEpPtb7f5rTn9F6n2y9/aij6R3LpDaHyxd+EnVcx4/QCrM+u1r79pW64+JjY3Veeed5wLsrbfe6gKsscBdXl7uwrYF1kGDBrlQ3KhRI33yySc699xz1blzZw0ZMqRWAfWUU05RRkaGfvrpJ23btq3a/G+fhg0bunZYCLXgfMkll7hjN910k84880zNmzfPBduvvvrKPT81NfU371FQUKAxY8Zo2LBhboj7pk2bdPHFF7sAHHhh4ZtvvnGB2PbLli1z72/B2T6zNv75z3/qkUce0XPPPacBAwbopZde0gknnKD58+era9eueuKJJ/Thhx/q7bffduF6zZo1bjPvvfeeHnvsMb355pvq3bu3Nm7c6C4m1BVCdyjrdJheXpCtH8s76OJgtwUAAABAnfjTn/6khx56SN9++60riOYbWn7qqae6YGvbn//8Z//zr776an3xxRcuUNYmdFtIXrRokXuNBWpz//33/2Ye9m233Vatp9w+04KphW7rtW7QoIG7SGDDyWvyxhtvqKioSP/5z3+UkpLijv3rX/9yc9f/8Y9/uOBvmjRp4o7HxMSoR48eOvbYYzVx4sRah27rJbeLEH/4wx/cfXtvC/DWu//UU09p9erVLnwffPDB7kKG9XT72GP2NYwePVpxcXEulNfm+7inCN0hLGnAabp7fEN3u7CkTMnx/LgAAACA3fb/1u/+a2IC5ij3ON77HlE7zM69bu7et83evkcPDR8+3PXWWui2nl8rombDp431eFtItpC9bt06N3S6uLi41nO2Fy5cqLZt2/oDt7Ge6B299dZbrof4119/db3rZWVlrmd9d9hn9evXzx+4zYgRI1xv++LFi/2h23qYLXD7WK+39a7XRm5urtavX+/eN5Dd9/VY2xD2I488Ut27d3eF6mwY/1FHHeUeO/300104tyH09tgxxxzjLgrYBYW6wJzuEJYcH6OEWO+PKCufYmoAAADAHolP2f0tJiCA2W07FrfDUr41vXYP2NxtG/acl5fnerlt6Pihhx7qHrNecBtObT271ps7a9YsN4Tbwve+MnXqVJ199tkugH788ceaOXOmG+6+Lz8jkPUwB7LeaAvm+8rAgQO1YsUK3Xvvvdq+fbvOOOMMnXbaae4xuwBhFwCefvpp14N/xRVXuPnuuzOnfHcQukNYlKdCXZIL1S4qU1lUMAcAAAAiloXC6OhoNzzbhmbbkHPf/O4pU6boxBNP1DnnnON6ka2HdsmSJbV+7549e7r5zLa0l8+PP/5Y7Tk//PCDG4JtQdsqptvQ7FWrVlV7Tnx8vOt1/73Pst5mm9vtM2XKFPe1Wa/zvmC979Zrb+8byO5bkbjA59lc8RdeeMH14ttFjezsbPeYhW3r3bae/UmTJrmLDrXtad9djFcOZRvn6JOSP2lDfJoWFhwb7NYAAAAAqCM2X9oC4i233OKGT9vwaB8LwO+++64LxjYX+tFHH1VmZma1gLkrNnfZqpKff/75rtfc3t/CdSD7DJvrbHO4Bw8e7Iq1jRs3rtpzbJ639R5bT7tVDbciazsuFWa95Xfeeaf7LKsQvnnzZjcH3Qq/+YaW7wt/+ctf3OfYiAArwGajA6xdr7/+unvcvkc2ZN2KrFngt8J0No+7cePGrqCbXTwYOnSoG6L/2muvuRAeOO97X6KnO5QlN3W7NOUqK6842K0BAAAAUIdsiPnWrVvd0PHA+ddW4MyGS9txm/Nt4dGW3KotC50WoG2YtRUMs2ri9913X7XnWOXv66+/3lUZtxBrAd+WDAtkhd1sDvThhx/uljnb2bJlFmKtYJv1KFt4P+200zRq1ChXNG1fsiW+brjhBt14441uCTWrqm7Vyu3igbELAg8++KDrtbd2rFy5Up9++qn7Xljwtt5vmwNua6BbobmPPvrILV1WF6I8tkhZPWKLztsYfhteYVdnQlpJoXR/S3fzpUO+059G9Q12iwAAAICQZBWzrRe2Y8eObl1moK7Pq9pmS3q6Q1l8skqjvcM1irZtCnZrAAAAAAC7Kaih+4EHHnBd/db137x5czdEwqrI7YqNv7eCAoFbJF/JKopr4vYleZuD3RQAAAAAQDiFblv8/corr3SV8yZMmOBKtNvaaYGV7nbGqtBZ5T3ftmNVvUhSmpDm9p78LcFuCgAAAAAgnKqX22T3HXuxrcd7+vTpbp20mljvthUPqA8qktOlXClqe1awmwIAAAAA2E0hNad727Ztbp+W5u3drUl+fr4r526T1m29uvnz59f43OLiYlcS37fZYvPhJDrFW0Evrsi7nhwAAACAmlVUVAS7CYggFfvgfIoNpS/muuuuc2Xb+/TpU+PzbEH1l156yZV2t5D+8MMPa/jw4S5476xinM0bv/vuuxWuYhs2c/v4kpxgNwUAAAAIWfHx8W45qPXr17vlrOy+jZAF9oQt8lVSUuLWGbfzys6nsA/dNrd73rx5+v7773f5vGHDhrnNxwJ3z5499dxzz+nee+/9zfNtcXlbv81n3bp1tV5EPhQkNPKG7kYV27S9pFxJ8THBbhIAAAAQciwY2bJOVvPJgjewL9i64+3atXPnV1iHbluA/eOPP9bkyZN3e+3suLg4DRgwQMuWLdvp4wkJCW7zsSHm4SS+UXO3T4vKU1ZBsdrEJwe7SQAAAEBIst5IC0hlZWUqLy8PdnMQ5mJiYhQbG7vXIyZig91lf/XVV2vcuHGaNGmSuzK1u+wf09y5c3XMMccoEkVZITUXunOVXVCiNk0I3QAAAEBNLCBZx5xtQCiIDfaQ8jfeeEMffPCBW6t748aN7nhqaqqSkpLc7fPOO0+tW7d2c7PNPffco4MOOkhdunRRTk6OHnroIbdk2MUXX6yIlNLU7ZooT6vyS4LdGgAAAABAuITuZ555xu0PO+ywasdffvllXXDBBe726tWrq42f37p1qy655BIX0Js0aaJBgwbphx9+CKt52rulxQG6s/kT+mq1R9cXELoBAAAAIJwEfXj577Fh54Eee+wxt9UbCQ2Vm95P61avU3ZBcbBbAwAAAAAI13W6sXNpKd7y9Fn0dAMAAABAWAmJ6uXYtUO2faj02PnK2nqepJ7Bbg4AAAAAoJbo6Q4DA9a/qStiP1T8tpXBbgoAAAAAYDfQ0x0GtnQ4Xu/NXqLVJQ2C3RQAAAAAwG4gdIeBnCE36J7pP6htsXcZNQAAAABAeGB4eRhI9xVSY51uAAAAAAgr9HSHgbSkKDVTjlTiUVFpuRLjYoLdJAAAAABALdDTHQYaznlZPydeodvjXmPZMAAAAAAII4TuMBCV3NTtmyhP2QwxBwAAAICwQegOBynpbpcelactBcXBbg0AAAAAoJYI3eEg2Ru6m0TR0w0AAAAA4YTQHQ4qh5enKVfZ+fR0AwAAAEC4IHSHUU93QlSZ8vJygt0aAAAAAEAtEbrDQXyySqMT3c3i3E3Bbg0AAAAAoJYI3WGiJL6J25fnbQl2UwAAAAAAtUToDhNlSWlu7ynICnZTAAAAAAC1ROgOs3ndMUWEbgAAAAAIF4TuMBGT4q1gHle8NdhNAQAAAADUEqE7TMQ1aub2DcpzVFRaHuzmAAAAAABqgdAdJuIbekN3E+Upu6Ak2M0BAAAAANRCbG2ehOCL6n+Wzv+uoeYXNVKfghK1apwU7CYBAAAAAH4HPd3hIrWNNjXsrS1KVRY93QAAAAAQFgjdYSQ9Jd7ts/KLg90UAAAAAEAtMLw8XBTl6rTi9zU4dpOyC3oGuzUAAAAAgFqgpztclJfqpM3P6trY97U1rzDYrQEAAAAA1AI93eEiqbEWNR+rKeulbfmEbgAAAAAIB/R0h4voGM0Y9KDuLTtXG7dHBbs1AAAAAIBaIHSHkTRfITWqlwMAAABAWGB4eRhpliQ111YV53uC3RQAAAAAQC3Q0x1Geky5TtMSr9Swgm+C3RQAAAAAQC0QusNIXIOmbp9SlqPisvJgNwcAAAAA8DsI3WEkrlFzt28Slads5nUDAAAAQMgjdIeRqJR0t0+PylVWPqEbAAAAAEIdoTucJHuHlzcRPd0AAAAAEA4I3eEk2dfTTegGAAAAgHBA6A4nlcPLbU73lvziYLcGAAAAAPA7CN1h2NOdplxlE7oBAAAAIOQRusNwTndCVJny87YFuzUAAAAAgN9B6A4n8ckqi050N0vzNgW7NQAAAACA30HoDjOliWluX56/JdhNAQAAAAD8DkJ3mKmoDN1RhVnBbgoAAAAA4HcQusNMVAPvvO7Y4uxgNwUAAAAA8DsI3WGmbNTdOrb4Pn1YNFAlZRXBbg4AAAAAYBcI3WGmQdt+WhTVSXlKVnZBSbCbAwAAAADYBUJ3mImOjlKT5Hh3O6uAtboBAAAAIJTFBrsB2E1bluqy2I+0KCZB2QVDgt0aAAAAAMAu0NMdbjYt1MVFr+ismK8ZXg4AAAAAIY6e7nCT3kU/NTpKk7LS1Cyf0A0AAAAAoYye7nCT0Uufdr5Tz5SfwJxuAAAAAAhxhO4wlJaS4PYMLwcAAACA0EboDkPNkqUMZSsnrzDYTQEAAAAA7AKhOwydNulI/ZR4lRJzlwe7KQAAAACAXSB0h6HypDTvjYItwW4KAAAAAGAXCN3hKDnd7aKLsoPdEgAAAADALhC6w1BMg2Zun1Sao9LyimA3BwAAAABQA0J3GIpr2NTt05SnrVQwBwAAAICQRegOQ1Ep3tDdJCpPW/IJ3QAAAAAQqgjdYTynOz0ql7W6AQAAACCEEbrDOHQ3UZ6yCoqD3RoAAAAAQA0I3eEo2Tu8PD0qj55uAAAAAAhhhO5wlJzmn9OdxZxuAAAAAAhZQQ3dDzzwgAYPHqyGDRuqefPmOumkk7R48eLffd0777yjHj16KDExUQcccIA+/fRT1SuVhdTSlKusfIaXAwAAAECoCmro/vbbb3XllVfqxx9/1IQJE1RaWqqjjjpKBQUFNb7mhx9+0FlnnaWLLrpIM2fOdEHdtnnz5qm+zelOiCpTQX5OsFsDAAAAAKhBlMfj8ShEbN682fV4WxgfOXLkTp9z5plnulD+8ccf+48ddNBB6t+/v5599tnf/Yy1a9eqbdu2WrNmjdq0aaNwVX5vhmLKi3Rl05f01FWnBrs5AAAAAFCvrK1ltgypOd3btm1z+7Q075zlnZk6dapGjx5d7diYMWPc8Z0pLi5Wbm6uf8vLy1MkWDTqZR1bfL+WFTUKdlMAAAAAAKEeuisqKnTddddpxIgR6tOnT43P27hxozIyMqods/t2vKZ546mpqf6tV69eigTxnQ/RfE8HbSwImYEKAAAAAIBQDd02t9vmZb/55pv79H1vueUW14Pu2xYsWKBIkJYS7/bbtpeqtLwi2M0BAAAAAOxErELAVVdd5eZoT548+XfnWbdo0UKZmZnVjtl9O74zCQkJbvOxIeaRoPGmn3R57IeaUd5VWwtHqXnDxGA3CQAAAAAQSj3dVsPNAve4ceP09ddfq2PHjr/7mmHDhmnixInVjlnlczten8Qs/lQ3x76pw2JmKbuAtboBAAAAIBRFB3tI+WuvvaY33njDrdVt87Jt2759u/855513nhsi7nPttdfq888/1yOPPKJFixbprrvu0i+//OLCe73Sdoi+jDtC8yo6Kiuf0A0AAAAAoSioofuZZ55x86wPO+wwtWzZ0r+99dZb/uesXr1aGzZs8N8fPny4C+nPP/+8+vXrp3fffVfjx4/fZfG1iNTnFP276U36pOIgZdHTDQAAAAAhKahzumuzRPikSZN+c+z00093W33XtIF3rnp2fnGwmwIAAAAACOXq5dh9zZM8aqatzOkGAAAAgBBF6A5Xm5fozjlH6KuEv2gLoRsAAAAAQhKhO1wlp7ldalShcvIKg90aAAAAAMBOELrDVVITeRTlbpbmbQl2awAAAAAAO0HoDlfRMSpLaOxuVhQQugEAAAAgFBG6w1hFUrrbR23PCnZTAAAAAAA7QegOY9Ep3tAdX7xVZeUVwW4OAAAAAGAHhO4wFtuwmdunReVpa2FpsJsDAAAAANgBoTuMRSV7e7rTlMda3QAAAAAQggjd4SylqdulReUqq6A42K0BAAAAAOyA0B3OfD3dUXnKyqenGwAAAABCDaE7nCVX9nQrl+HlAAAAABCCCN0R0dOdryxCNwAAAACEHEJ3OKtcMqxJlBVSY043AAAAAISa2GA3AHshvau+HPScHvlhqzoxpxsAAAAAQg493eEsoYGK243UYk87hpcDAAAAQAgidIe59Abxbk8hNQAAAAAIPYTuMNdhw+e6IuYDJeSvC3ZTAAAAAAA7YE53mGs25zndFDdHC4vbqbzCo5joqGA3CQAAAABQiZ7uMBfT/Wi9XXaoNlekamshQ8wBAAAAIJQQusNc9Khb9UD8VZrn6cS8bgAAAAAIMYTuCJCW4i2mlsWyYQAAAAAQUgjdEaBlskepyldWQXGwmwIAAAAACEAhtXA3/VW9lnmNJsQN1IaCocFuDQAAAAAgAD3d4S6pidulReUxvBwAAAAAQgyhO9ylNHW7NOVSSA0AAAAAQgyhO9wlp/t7ugndAAAAABBaCN3hLtnb050aVaiteQXBbg0AAAAAIAChO9wlNZZHUe5mWX5WsFsDAAAAAAhA6A530TEqT/QWU1MhoRsAAAAAQgmhO4LmdccWZam8whPs1gAAAAAAKhG6I0B0ijd0N1aecgoppgYAAAAAoYLQHQGifcuGUcEcAAAAAEIKoTuSlg1TnrII3QAAAAAQMgjdkcDf051LTzcAAAAAhBBCdyT1dEflKSu/ONitAQAAAABUivXdQBjrebxeWJqi/y6s0Cn0dAMAAABAyKCnOxI0bqfcFsO02pPB8HIAAAAACCGE7giRlhLv9hRSAwAAAIDQQeiOBKXbNWDzeF0RM15ZeUXBbg0AAAAAoBJzuiOBx6P+s+5S/zjpxIJTgt0aAAAAAEAlQnckiE9Wbsdj9NnSQuUW0NMNAAAAAKGC4eURoujkl3Vz2aVaVRivigpPsJsDAAAAACB0R44mlYXULG/nbC8NdnMAAAAAAITuyBEXHaWMxDIlqUjZBcXBbg4AAAAAgNAdQT64Uj/pPJ0dM1FZ+SwbBgAAAAChgNAdKRJT3S49KlfZrNUNAAAAACGB0B0pktPcronylEXoBgAAAICQQOiOFMlN3S49Ko/h5QAAAAAQIgjdkSI53e3S3PByCqkBAAAAQCggdEeKFG9PN8PLAQAAACB0ELojrKebQmoAAAAAEOahe82aNVq7dq3//rRp03Tdddfp+eef35dtwx7M6U6NKlROXmGwWwMAAAAA2NPQ/cc//lHffPONu71x40YdeeSRLnjfeuutuueee/Z1G1EbSY3lUZS7WV6QFezWAAAAAAD2NHTPmzdPQ4YMcbfffvtt9enTRz/88INef/11vfLKK/u6jaiN6Bh5krzLhkUXZauiwhPsFgEAAABAvbdHobu0tFQJCQnu9ldffaUTTjjB3e7Ro4c2bNiwb1uIWouqnNfd2LNNuUWlwW4OAAAAANR7exS6e/furWeffVbfffedJkyYoKOPPtodX79+vdLTvcEP+19UQAXzLazVDQAAAADhGbr/8Y9/6LnnntNhhx2ms846S/369XPHP/zwQ/+wcwRBsnd4eVpUHhXMAQAAACAExO7Jiyxsb9myRbm5uWrSpIn/+KWXXqrk5OR92T7sjkNu1J2ZB+vzDak6uKA42K0BAAAAgHpvj3q6t2/fruLiYn/gXrVqlR5//HEtXrxYzZs339dtRG21Hqj1TQZri1KVRU83AAAAAIRn6D7xxBP1n//8x93OycnR0KFD9cgjj+ikk07SM888s6/biN2QnhLv9lnM6QYAAACA8AzdM2bM0CGHHOJuv/vuu8rIyHC93RbEn3jiiVq/z+TJk3X88cerVatWioqK0vjx43f5/EmTJrnn7bjZWuGwKyBrdET+xzop+nvmdAMAAABAuIbuwsJCNWzY0N3+8ssvdcoppyg6OloHHXSQC9+1VVBQ4IqwPfXUU7v1+TaM3ZYm820Maa+0ebGOWvEPXRr7CcPLAQAAACBcC6l16dLF9UqffPLJ+uKLL3T99de745s2bVKjRo1q/T5jx4512+6ykN24cePdfl3Ea9xOG1ococlrU5RNITUAAAAACM+e7jvuuEN//vOf1aFDB7dE2LBhw/y93gMGDFBd69+/v1q2bKkjjzxSU6ZM2eVzreCbVVn3bXl5eYpYzbpp6RHP6+9lZzGnGwAAAADCNXSfdtppWr16tX755RfX0+0zatQoPfbYY6orFrSfffZZvffee25r27atW77M5pjX5IEHHlBqaqp/69WrlyJZmq+QGsPLAQAAACDoojwej2dv3mDt2rVu36ZNm71rSFSUxo0b5yqg745DDz1U7dq103//+98ae7pt81m3bp0L3mvWrNnrNoeiDTmFGv33T1UeHa+F9x3vvq8AAAAAgH3LsrB1BP9ettyjnu6Kigrdc889rue4ffv2brM51vfee697bH+y4e3Lli2r8fGEhAQ3z9y3+QrARaqM/xys+YkXqbtnhXK3lwW7OQAAAABQr+1RIbVbb71V//73v/X3v/9dI0aMcMe+//573XXXXSoqKtJ9992n/WXWrFlu2Dm8ohO8FxXSovKUVVCs1OS4YDcJAAAAAOqtPQrdr776ql588UWdcMIJ/mN9+/ZV69atdcUVV9Q6dOfn51frpV6xYoUL0WlpaW7I+C233OKGg9v63+bxxx9Xx44d1bt3bxfurQ1ff/21K+CGSsnpbpcmC90l6tQs2A0CAAAAgPprj0J3dna2evTo8Zvjdsweqy0rxHb44Yf7799www1uf/755+uVV15xa3BbwTafkpIS3XjjjS6IJycnu6D/1VdfVXuPei+lqds1sZ5uKpgDAAAAQPiF7n79+ulf//qXnnjiiWrH7ZgF4dqyyuO7quNmwTvQTTfd5Db8fk93elSusqlgDgAAAADhF7offPBBHXvssa6X2bdG99SpU13Vtk8//XRftxF7ELqbKE9bCqqqtgMAAAAA9r89ql5uy3QtWbJEJ598snJyctx2yimnaP78+TUu3YX93dOdpy0MLwcAAACA8OvpNq1atfpNwbTZs2e7qubPP//8vmgb9qanOyqP4eUAAAAAEI493Qj9QmppYk43AAAAAAQboTtSlwxz63QTugEAAAAgmAjdkSbZ29PdOKpAOXkFwW4NAAAAANRruzWn24ql7YoVVEOQJTWWR1GKkkee7dluSbaoqKhgtwoAAAAA6qXdCt2pqam/+/h55523t23C3oiOkZLTpMIsNarIVW5RmVKT4oLdKgAAAACol3YrdL/88st11xLsM1GnPK8L/jtXazzNXDE1QjcAAAAABAdzuiNRl9FantJf25Wo7ILiYLcGAAAAAOotQneESkuJd/st+VQwBwAAAICwGF6OMLF+pk4r/0zxUY2VXXBAsFsDAAAAAPUWPd2RaOFHOif7SY2NmebmdAMAAAAAgoPQHYlaHKDFTQ7TYk9bZTG8HAAAAACChtAdiXqfrMkDHtOb5Ucoi0JqAAAAABA0hO4IL6TG8HIAAAAACB5Cd4RKT4lTirYzvBwAAAAAgojQHYm2rdPIt3trRsL/KTuf4eUAAAAAECwsGRaJkhoruqJUCVFSUeE2eTweRUVFBbtVAAAAAFDv0NMdieJT5IlNcjcbVmxTXnFZsFsEAAAAAPUSoTtCRaU0dfs05Smbed0AAAAAEBSE7kiVnOZ2aVF5yqKCOQAAAAAEBaE7UiVX9XRnUUwNAAAAAIKC0B2pktPdLi0ql7W6AQAAACBICN2Ryjenm+HlAAAAABA0hO4In9PdxAqpEboBAAAAICgI3RE+pzs9itANAAAAAMFC6I7wOd1NovK0hUJqAAAAABAUhO5I5V+nm0JqAAAAABAshO6Ir17O8HIAAAAACJbYoH0y6lZqW2059kVd/v5qZZWXyOPxKCoqKtitAgAAAIB6hZ7uSBWfrJR+p+hnTw+VlFUov7gs2C0CAAAAgHqH0B3BkuJjlBQX424zxBwAAAAA9j9CdyRb+pUuSfxKbaI2ad3W7cFuDQAAAADUO4TuSPbdI7qh9AX1jVquj+asD3ZrAAAAAKDeIXRHsg4jlNXuaGV5UvXR7A3aXlIe7BYBAAAAQL1C6I5kR9ymJhe8qfVNBrpCap/N2xDsFgEAAABAvULojnDR0VE6bWBbd/udX9YGuzkAAAAAUK8QuiOdx6NT+6XJluieujxLa7ILg90iAAAAAKg3CN2RbP446d6mavPphRrRuak79O50ersBAAAAYH8hdEey+IZSRZlUmK3TD2zjD90VFZ5gtwwAAAAA6gVCdyRLSffuC7doTO8WapgYq3U5290wcwAAAABA3SN0R7JkX+jOUmJstE7o18rdfeeXNcFtFwAAAADUE4TuSJbsncet8hKpOE+nH+itYv7ZvI3KLSoNbtsAAAAAoB4gdEey+GQpNsl7uzBL/dqkqmvzBiouq9DHs1mzGwAAAADqGqE70qVU9nZnzldUVJS/oNrbDDEHAAAAgDpH6I503cd695/cIOVv1skD2igmOkqz1uRo2aa8YLcOAAAAACIaoTvSjb5Latpdys+Uxl+mZilxOrx7c/fQO7+wZjcAAAAA1CVCd6SLT5FOf0WKTZSWfSVNfdI/xPz9metUVl4R7BYCAAAAQMQidNcHGb2ko//uvT3xHo1qsErpKfHanFesb5dsDnbrAAAAACBiEbrri0EXSL1PlirKFDvnDZ00oLU7TEE1AAAAAKg7hO76IipKOv6f0rGPSMc+pjMq1+yeuHCTsvKLg906AAAAAIhIhO76JDFVGnyxFB2t7i0aqm+bVJVVeDR+1vpgtwwAAAAAIhKhu74qKdDDiS+pR9RqvfPLGnk8nmC3CAAAAAAiDqG7vvrqLnVb+56eiP+XlmzcpnnrcoPdIgAAAACIOITu+urQv0qtB+mzdn9WhaL1znQKqgEAAADAvkborq9S0qWLJ2rAIce5ux/MWq+i0vJgtwoAAAAAIgqhuz6LitKILk3VMjVR6UWrNPXnn4LdIgAAAACIKITuei4mOko3dlqjj+JvVadvrpJKi4LdJAAAAACIGIRuaOjQQ1SkeLUv/VUFn9wS7OYAAAAAQMQgdENtO3TWc2k3udsps16SFn4U7CYBAAAAQEQIauiePHmyjj/+eLVq1UpRUVEaP378775m0qRJGjhwoBISEtSlSxe98sor+6Wtka7riJP1XNmx7rbngyulnNXBbhIAAAAAhL2ghu6CggL169dPTz31VK2ev2LFCh177LE6/PDDNWvWLF133XW6+OKL9cUXX9R5WyPdMQe01NPRf9Ssis6KKtomvXuRVF4a7GYBAAAAQFiLDeaHjx071m219eyzz6pjx4565JFH3P2ePXvq+++/12OPPaYxY8bs9DXFxcVu88nLy9sHLY88KQmxGtO3ra6afrUmJN2mpLXTpG/uk0bfFeymAQAAAEDYCqs53VOnTtXo0aOrHbOwbcdr8sADDyg1NdW/9erVaz+0NDydfmBbrfU01y1ll3gPfP+YtGxisJsFAAAAAGErrEL3xo0blZGRUe2Y3c/NzdX27dt3+ppbbrlF27Zt828LFizYT60NPwe2b6KOTVM0vmSwfm1/hvfguP+T8jKD3TQAAAAACEthFbr3hBVca9SokX9r2LBhsJsUsqyY3WmD2rjbdxSfLTXvLRVslsZdKlVUBLt5AAAAABB2wip0t2jRQpmZ1Xtd7b6F6aSkpKC1K5KcMrC1oqOkKSsLtG7001JcsrR8kvT9o8FuGgAAAACEnbAK3cOGDdPEidXnGE+YMMEdx77RMjVJh3Rt5m7/b0WSdMxD3ge2bw1uwwAAAAAgDAU1dOfn57ulv2zzLQlmt1evXu2fj33eeef5n3/ZZZdp+fLluummm7Ro0SI9/fTTevvtt3X99dcH7WuIRKcf6B1i/t6MtSrv+0fpkm+kMfd5H/R4vBsAAAAAILRD9y+//KIBAwa4zdxwww3u9h133OHub9iwwR/AjS0X9sknn7jebVvf25YOe/HFF2tcLgx7ZnTPDKUmxWnDtiJN+TVLaj2w6sEV30pP9JemPh3MJgIAAABAWAjqOt2HHXaYPLvoNX3llVd2+pqZM2fWccvqt8S4GJ3Uv5VenbpK70xfq5HdvMPNnTnvSFtXSllLq47Zz7CsSIpjXj0AAAAAhO2cbuzfNbvNF/M3althadUDxzwonfycNLhyLW+zZpr0cDfpo2ulNT8z/BwAAAAAKhG6sVO9WzVSjxYNVVJWoQ9nr6t6ID5F6vcHKaNX1bFFH0nFudL0V6R/j5aeGiJ9/5iUuyEobQcAAACAUEHoRo1rdvt6u22I+S6Nvkc670Op7x+k2CRpyxLpq7ukx3pJr58uzR8nlRXvn4YDAAAAQAghdKNGNq87LiZKc9Zu0+KNeTU/MTpa6nSodMpz0p+XSMc/IbU9SPJUSEu/lN65QHqku/TpX6T1Mxl+DgAAAKDeIHSjRukNEjSqR4a7/c4va2r3osRG0qDzpYu+kK6aLh1yo9SwlXed72nPS88fJj0zXNo4t+o15aUEcQAAAAARidCNWq3ZPW7mOpWWV+zei5t2kUbdIV0/TzrnPanPqVJMgrRpgRSXXPW8Kf+U/tFBmvSPqmMV5VLuesI4AAAAgLAW1CXDEPoO7dZMzRomaHNesb5etEljerfY/TeJjpG6jPZu1uO9bKLUuH3V45sXS0U5Ukxc1bHs5dK/DpQSUqVm3Su3HlLzHt59o9Y28XzffJEAAAAAUEcI3dil2JhonTKwtZ77drne+WXtnoXuQElNpANOq37shCelEdd6H/PJWSVFxUjF26S107xboPiG3iDe5kCp/XCp/QgppenetQ0AAAAA9rEoj6d+jd9du3at2rZtqzVr1qhNG+/Qaezask35Gv3ot4qJjtKDp/ZV5+YN1DE9RanJAT3TdcEqnmf9Km1eFLAtlrKWSRVlv31+0+7SpZOk+ICh6wAAAAAQxGxJTzd+V5fmDTSwXWPNWJ2jG9+Z7T/eJDlOHZumqEPTFBfC3b7yfoOEfXBqxSZ41wMPXBPclJV4h59nzpNW/yit+kHaNF+Sp3rg/uBK7374tVKzbnvfHgAAAADYTYRu1MrfT+2rl6es0PLNBVqZVaDM3GJtLSzV1tU5LozvyOaBe4N4sjo2baCOTZNdGO+QnqLEuJi9a0xsvHdut22+oeqF2VLuuuoV0ee9L5UWSgdVhm+zYrI3sNtw9PQuzAsHAAAAUKcI3aiVbhkN9cApff33C4rLXPheuaXQ7VdssdvefVZBiSu8Ztu0ldm/ea82TZJ0wfAObrM54/tEcpp3C3T6q9654FZ4zWfGf6W5b3tvpzT3zgfvcLDUvJeU2tq7vJmFegAAAADYB5jTjX0ut6jUH8B9oXx5ZSjftr3U/7xeLRvp/lMOUP+2jfdf4358Rlr4kbT2F6m8eCdPiJIaNPdWR7cQ3qiN1GWU1PVI78MVFZKnQorhehUAAABQn62tZbYkdGO/2lpQos/nb9TfP1vkAriN7j5naHv95ejuapRYx4XZApUWSetnSCunSKt/kLJXeNcF31kQP+RG73rj7gtYKT0xwLvk2TUzq4anL/7MW/jNetWtqjrD1gEAAICIRiE1hKQmKfE6a0g7HdkrQ/d/slDvz1yn//64ygXxO47rpeP6tlTU/giscYmVS40Nrzpm158Ktki5a70BfNs67+0Oh1Q9x45ZT7cJbOe3/5DWz/TebpAhdRwpdTxU6nSo1Lhd3X89AAAAAEISPd0Iqh9+3aLbxs1zw8/NyG7NdO+JvdU+PUUhyYaXF2ySinKrV0T/+Hpp41zvVlZU/TVNOnrDt4VwC+OsJw4AAACEPYaX14DQHXqKy8r17KTlemrSMpWUVSghNlpXH9FFl47srPjYfVRobX8OW7fibcu/lVZ8K62bIXnKqz8n4wBvCD/wT1J652C1FAAAAMBeIHTXgNAduqzw2m3j52rKsiz/+uD3ndRHQzulK2xZj/iqKZUhfHLleuKVLvlGaj3Qe9t6yIu2SW0Ge9cnBwAAABDSmNONsNOxaYpeu2ioPpi1Xn/7ZIGWbcrXmc//qNMHtdEtx/RUWkoYLuWV2EjqPta7mfxN3vC9eqrUsl/V8358Vpr1mjTiWunIe7zHtq2VVv0gJaVJSU2kpMbefWJjKTrMRgAAAAAA9RShGyHFiqidNKC1Du/eXP/4YpHe+Gm13pm+Vl8tzHTB2wL4fim0VldsObIDTvNugRJTpZRmUoeRVcfW/iy9f8lO3iSqKoD7A3nlZmuVj/yLFB1TVW3dBrM0bCHFJdXt1wYAAADgNxhejpA2fdVW3TpurhZtzHP3h3RM0/0n91GX5g0VCsrKKzRzTY6+XrRJ3y/dovQG8frr2B7q0aLR7r+Z/VO0yui+wPzr19L3j0vbs6XtOdL2rVJJ/q7fIzZRui2z6v4bf5CWfCYd97h04IXeY2unSxPvkpLTpeSmlft0KaVy7z+exlB3AAAAoAYML0dEGNS+iT66+mC99P0KPf7VUk1bka2x//xO/zeys646oosS4yoD6n6UU1iib5dsdkHb9jmFpdUet/B90SEdde2orkqO341/YtaDHxXw9XQ+wrsFKivxhu9qW3bV7Yqy6s+3AG9B3IK0/wtY5R3iXhvxDb1h3HrUL5ogxVR+PXPelrKXS92Ollr19x6zOelZy6SEVCmhoXdovX12OI9MAAAAAPYSPd0IG2uyC3XXh/M1cdEmd79dWrIbbt41o6G6ZjRQ+7Rkxcbs+7nO9k9kcWaeC9nfLNrket8rAv7VpCbF6bDuzTSyazNNWJDp1hw3rRsn6Z4Te2tUzwwFnf0z94XfnDXeOeWFWd7N1iZ3t7OlwoDbgVXXf9ODfqa05HPp+CekQed7jy2bKL12SvXPjY6rCuAJjbzD6HfcbB67b+i7BfmSQim1tXe4PAAAABCi6OlGxGmblqwXzz9QX8zfqLs+XKDV2YV6ZMIS/+O2vFinpinqZiG8eQMXxrtlNHDhfHfD+PaSck1dvqUyaG/Wupzt1R7v0aKhDu/RXEf0aK4BbRv73//UQW00cWGm7vhgvnvNRa/+ojG9M3TXCb3VMjWIc6oDe5sbt/Vuv7ceefE2qaAymBd7h/f7dRsjNWwpNe9V/XijNt7nFuda0pcqSit74rNr/qwR11XdnvQPac6b3mJyFsbNhtnSW+d4C8j5w3pjqXE771rpTbt7l15jKDwAAABCEKEbYcWKqB3dp6UO7tpMb/28RvPXb9PSzHxX6Xx7abmb++2b/72zMG4h3OaD2759eopioqvC6Nqtha4n24L2D79mqbiswv+YrR0+oktTf9C2XuyaWM/2sM7p+ufEpXrxuxX6Yn6mG3J+w1Hddf6w9nXSG7/PWXV0X3E2dfnt47bG+I66jJJumF8V2m3+uYVvWzbN9hbGbQh64GbPiUuseo/4ZCmluXdOuY+F/pzVkmyrgQ3Lb9JBatZdatrNu+99MsXjAAAAEHQML0dEqKjwuJ7lJZl5WpKZr6WZeVq6KV9LN+WpqLQqPO8Yxjs3a6BOzVK0LDPfDSEPZMH68B7NXMge1qmpkuJ3f/74oo25+n/vz9WM1Tnufu9WjXTfyQeof9vGe/iV1kMWzjcvqQzqOdqStVk/L/hVzcvWq2vUOjXMX64o17MeICpa+n8bqgL91Ke9ldz7nSm1HhSULwMAAAD1M1sSuhHxYXzt1u0ufPvC+JJNea5nfMcwbp3eVrjN15vdPaPhPlmezNrw1i9r9MCnC5VbVOZGep97UHv9eUx3NUqM2+v3r0/GzVyr28bNU0FJ1XzzBgkxOr5zjI5rmauByZuVlLPM27N+yvNVL/z3UdKan6RT/121XJsVk/v6b1JaJym1jdSotZTa1juf3O7bXHQAAACgBoTuGhC6ERjGrWd8+ZZ8ZTRK1KHdmqlxcnydfeaW/GLd98lCjZu5zt1v1jBBdxzXS8f1bRnea4/vBwXFZbr9g3l6f8Y6/9JxNkrB1m/fnFfsf15sdJSGdkrTkT0zdGTvFlXTAOa+K22Y5R0WbyHb/PAv6ctba/5Qmztuc9QtgPuCeOP2v11jHQAAAPXSWkL3zhG6EWxTlm3RbePnacWWAnd/ZLdmuvfE3m6OOX5r3rptuuZ/M7V8S4EbjXDtqG5uuTibj28XT2avzXFV422zKQWBerVspCN7ZbjNhvZXu7hh88TXTPMuobZtrbRtnXefu9Y7lH1nrCf8+nlV9988W8rPlMY8ILUdXFX4beX3UlyyFJ9SubetQdXtuJTKfXLVuuwAAAAIK4TuGhC6EQqKSsv13LfL9dSkZSopq3CF2q4+oosuHdnZzTWHd6m2l6es1N8/W6SS8gq1TE3U42f219BOAWuO78AuZHxVGcB/WZVdbWk36/Ue3bO5juzVwvWGx+2qoJ0VfQsM4b5QbkPOj3mw6nmP9vY+fvFEqc2B3mNTn5K++H+1/0JtOTbrQb9qWtWxD66Utiz1VnFvd5D32PJvpZ+e9c5Xj44N2GIqt1hvQTnfsZh4b2/98Gu8hfGMfR0mpRnV3gEAAPYSobsGhG6EEguJt42fqynLstz9Ls0b6G8n9dFBuwiW9UF2QYluene2vlroXZPdeqofPLWvmqTUfvh/Vn6xq0RvAXzy0s3V5vA3TIzV4d0tgGe4OfwNEvZwIYf1M73rnnc6zLsWuVnyhTT3He9646UFlftCqcRuF1TdtiXVfKzy+rWzq+4/d6h3OPwf35G6HeU9NvM1bxjfHbFJ0m3edeNrXF999U/ShNu9y7D5KtYnBd5Ok5q0914YiK276RcAAADhhtBdA0I3Qo39E/xw9nrd+/ECbckvccdO7N9KfxzSToM7pCk6YFmz+uDH5Vm67s1Z2phbpPiYaN16bE+dN6z9Xs17t5EFtmybBfCJizL932fTJDlO1x/ZzX2/99tybvZrt6yoKphXlFXNNTc2PH37VqnNEKlhhvfYlmXSqine53oqvHv/Vl65Vd73lEtllXPdx/6jeuheNlE67SWp1wneY/Pek97dyRJwO7IedlsbPa2zd130I++tvtwbAABAPbOW0L1zhG6Eqm2FpfrHF4v0xk+rqw2JPr5fK508oLW6t4jsatpl5RV64utl+tfXS92wcFvK7cmzBqh3q9R9+jnlFR7NWrNVXy7I1GdzN2p1dqE73rV5A912XC9XUC+i2a9823xDznPXS+ume0O+23KqbhflSAVbpOwV3osDPjY//Za1cqX4zTsXSBvnSUf9Tep+tPeYb332hq2qPgsAACCCELprQOhGqJu9Jkev/bhKn8/bqLziMv/xHi0a6qQBrXVCv1Zq5avKHSHW52x3vdvTVma7+6cPaqO7T+yt5Pg9HPa9G0H/f9NW69EJS7S1sNQdO6x7M912bE91aR7ZFzl2i/1vwgrGZf0qZS3zDo8fdkXV408NlTYvks55T+oy2nts9lvSuEu9Q9zTOnp78m0YvQ1Ttx7zxm29e5ZmAwAAYYrQXQNCN8KFDYmeuHCTxs9ap0mLN6m03PtP1ToXh3RIcwH8mD4tlZoc3mt9fzl/o256b45yCkvd3Or7Tu6jE/u33u+jDJ78eqle+WGlyio8rjK6raV+7aiuuzWPvN6yAm0Wxlv2984HNz8+I315m3e4+67YvHEL31YZvlkPadTtVY+VFjGEHQAAhCxCdw0I3QhHOYUl+nTuRhfAp63w9gYbm/N8eI9mOql/a1cQLDEuJqwuKjzw6UK9OnWVu39A61Q3nLxD0+AtnbZ8c77u/3SRW//bpCbFueB97rD2u652jp0rL/MuyZa93BvKreic3bfl2rat8Q5hD9S8l3TF1Kr7z4zwvuaPb0nth3mPbV4sbZjjLVxn1dkTKve22RJtrHkPRBarT2G/K2yETUpT7791U5gtZc73Lr3YZlDV8xd+7F32sbxEKi+VKkorb5d591YTw1Z3iImr3MdLbYdIrfp7X2+vXfWD9/dJx5FV72sjfewiou91NorHLhoyfQao19YSuneO0I1wt3ZroSu89sHM9VqcmVetIvfYPi1cALdltay3NlQt25Svq/83Uws35Lr7lxzSUX8Z0yNklkuztdStsN2ijd7vr80vtyHnVvF8bwq6YQc279vCtwvjq7292gPPq3r8gbbeeeFX/iw16+Y99u1D0jd/2/n72ZJpFsYDg7htNrT9qHurnrfoU+8fz+1HSCmVKwVYUTsrQGdrqPNHdGha9Im3BkGHQ6TmPbzHcjdIyyd5l8mzYn9uST3f7Zjqx/2PxUhth0oxsVVFCm36hG/ag2/ZQFudwPzmz6Qd7ge+f6sBUlzl9B9bZrBgk5TSXEqtHL1jIdBqJLjnRwW81n6v7OJ3S4PmVcv82b8bC4YWNn3nr7XR/i0ZO4cTGoTWsoDWPgu8gW1a8KG0PTugnsRO6krYZis++Jz4lDTgHO/tpROk10/zjrD5v2+rnvN4X+/Fvd0x6k7pkBu8t9fPkp4/VGrUWrphQdVzXhglrful+uui46SGLSq3lpVbC6lRK+++aTfvbQCq79mybidMAtjn2jRJ1hWHdXGbhVbr/f5w1npt2Fakt39Z67YWjRJ1Qv9Wrgp6r5aNQiYo2jW+d6av1Z0fzNf20nKlpcTrkTP6uTAbSkZ0aapPrjlEb/28Ro98uVjLNxfoT6/8okO6NtVtx/aK+KJ2+43rre4tZfTe+eM3LvIGcpsT7mMhw3qfLHT4wocFc1/Vdt8f6oFaHFD9/pe3envf//RFVWiZ8ar0+V+9t+MbekOLzTePr9wHbr5jFoR8AcBsXuINZKltvL1kqF0Ys5/X1hXSVhsFsapqb+Hr0m+qnvvzv6VfJ3qXvPOFbuvpHH/Z7n/uX9dIMZXL/E15zLskX2DwspEZrx6/++97zcyqlQimPS9NeVw66Erp6Pu9xyzcPzV499/3oq+ktpWvm/Ef7znc90zplOereoMfP+C3gdDOY9/57M5b377yPLalA33//uzC14bZ3rDZemDVz2fxZ1LZdu90D7ff8fZ272oMFo59x0feJHU8xPse896X3r9U6nCwdN74qvZ9eJX3329t2IUJu5gQeGHCLq417e69WBLIPqegu/frd73ScdVv23vZxY9yXw94idSse9Xr7cJA6wO9veo7/r6ynu3A11kvul3s8F3w2NHwa6ou+OVtlN672FvX4qSnqp6zZakUm+gN7L4LQQAiDv+6gTDWs2Ujt908pocrQvbBrHX6ZM4Gt9zW85OXu619erKO7tPCzf/u2yY1KAG8osKj+etz9cJ3y10vvRneOV2PndlfGY1Cc86ujRT449B2Oq5fSz31zTK9/P1Kfbd0i8b+c7I7fv3obkpvEEI9SZHIgqsvXPkc+CfvFsiCgf3BHxjC3e0c720LHYFaDZRSmknJAX9UF+dX3S7J8255G3bdPvvjOTB0v39J5frqb0vdxniPzXlH+uIWb8AJDD32tfkCkf92A2/vpQUi35JuvjBkS8I1yJDikxWWbCjwmmmVoXpl9YBt3+uaWI+zr9iefU/t+2QF+XySm0idR3kvuLil9Cq8e7tv3zP/7YDjtrcebx/7vlqPpAUqHwtBVmPAr/L3pv/3p29fuRqAe+/KYcs+1m4LsL7h0D72Oe75Aa+zbVcCf29Hx3rbZ0EykB2z97EwaCwQ7uwiVCArfOgL3SsmSx9cKXU5Ujrn3arPfesc7/dtd/QP+Hdh57e1xf49Bup8hPdiQWJj7/fEbYG3AzYL2DuOQGk3VLpq2m8/+6SntVea95Qumfjb4+eOq36/rMQ7ksFGW+QFbhu9IzJsH7gUpNW+WPmdlLqy+vvY93zNT96LAbbag42KsAt3tjWq3Ps2+16EyEV0ALuH4eVAhCkuK9c3iza7AP71ok0qLquotgTZmN4tdMwBLTSwXZM6XQPc5qFPXrrFFYGbvGSLtuQX+8PsDUd202WHdg7pIfA7WpVVoAc+XaTP52/0D+e/5oiuOn94h5AZFo99sHa6hbzArSS/8rYtgZZf/ZiFBV8PpnnlOGnjXOmsN6vmoP/0nPTZTbvXFvvD+uaAP8ytx9UC0an/lg44rWqIvPXwWki34cz+feVt662zELbj/rD/VxVe7D0tGLQe5F173djFCuvl9b3GejH93wPf155fdcx3/+Rnq4LsZ3+V5o+TDru56gLJr99I/z2p5q+5QQupiVW2b+8N1b7bNgw8lmKGu8XmLtvPpaaflf9+gTTg7KpgaHOhp/zTG2Zt+b/A89r+fbhzy84L33mW5D1P3Lm3w/E2g70/Q2OfY8Hfzuv6PALEll+0fwd2YaTfmVXHXz7Ge0HKLkz8HvteW/gedqU06ALvMfuZ2rB3+/cSOCrIjSKIqppSYRdr3O3w+f8uEA4YXg7UUwmxMa5n27aC4jJ9s3iTPpu3Ud8s2qR1Odv10pQVbmveMMEFcJsHPqRjmmL3slCYrzfbPs+C9qw1OW69bZ/k+Bg3bNvC9qD2AT1KYaJ9eoqePXeQflyepXs+WqAFG3J136cL9fpPq/T/jumpI3tlhMwwfuwB+9n5AoMNG98TF3z822N9z/AOd3VhpzLo7Bh8AsOQhVzfnGCfmATv0NrAwGLPdUPst+3G1xgjHXFb1X27ILDoY+nYR6tCt81j/k9AL3ttHfuId/itr235G6XCrKrHLQy06Fs9WPv3bX/7NWPP2RBl12NcuZJAbfU8zrvV5rzeHXbe1uew7WPD1fue/tvjF1qNiQpvr7n1hgduuQG3CzZ7R/RsWeIdIeCzaaH0nxO9Q+0De/5fPFLasvi3nxdY78CCuLtt9Q5ivcPhR1zjfV7+Zu+Um0Ytq1+EsYJy9lwbLRSuI2+w79nIIjtH7UKuTaOxc9UVKkwIKD4Y793bBWtf/Qxj/y+0kTt2oTeC/44idAMRLCUhVsf1beU2qxb+7ZLNbv1vq869Ka9Y//1xldtsbvVRvTJcUB/euWmte263Flhv9mb3vpOXbNaW/MphjZW6ZTTQYd2b67BuzXRgh7SI6BE+qFO6Prr6YL03fa0e/GKxVmYV6tL/Tnfzva36euNkeuUQwDc8dm/4hvoG6n6MdNV0qbSgcl6tzaf1zbEt9P5Rbj33bqu8bX/Y7zjX3QJy4JxY+4PchrT6Xmc9mIHz2H1D5Hc8Zn+E+xxyozTkUu/Qah8L15d9t3ffByBSWej1FWRrc+DOn2Pz5XPXeQN44LB1m05ggXvHaQw1TQnwTWfYWc+6/bv3sc+a9653FEpg6P7gKmn1D97b9m/fLiZYsUAL4Q2aefe+zeap2+8X+7oCp3Qg/FgtA6vvYIH6wIuqRkx9ebs0+02pcMvvT5MJnFoSOF3jkZ7ei9JXz6i6AGxFU39+QTrjv97RNxGA0A3UE7acmPVs21ZSVqEpv27RZ3M3aMKCTGUXlOjNn9e4rVFirEb3ytDYPi1dkAxchsx6s+et36ZJizfvtDc7pbI324L2od2bueHskciGxZ8xuK2O6dtSz0xaphe+W+Hme5/+7FT956IhapkamV83QogLvl327j0OqywcF8iKX90wf+/eN3CIK4B9wy6AWSDxhRKfDiN2Prf9ip8q6xmUeXshXbHJioDblXUPXO2DymMWlANrHYy533shLpCFZ+u9LC+umsZgdRp2JbCYoBVItFE2diEucJg99l94tukeNhLJam3Y3lYR8N3P3+QdqWT79sOl4x6reu3btrqIR+p9clWhQbs4a6M0jJ0rdgGmodUfaVBVcNDqH5QHbMnpO7SpssMmsCaG1YCwgL/jahFhjDndQD1XVl6hn1Zk67N5G/T5vEz/3GtfiLb1v4d2TNPM1TmuV3vH3uzuGQ11WPdmLmQf2D4yerN316KNuTr/pWnKzC1Wq9REF7y7NKfCOQAgAll0sCHBNpzYNgtovtuB962gnPWYH3mvNOwK72vXTZdeOMLbC24rVPi8e5F3aLJbus+3tfXuXdHLwMKDHu/FAhve7pu6YOHPXm/BL3Dosq1UYSOAfL2w/hoXSVW1LlxV+xAe1myjlPzrzQdUz/fdtrn+vhoKFnCtNoMF6dF3V9XE+PwW7yoNVp+ktmylkPM/qrr/2mneiz9jH/JOO/B9f+1csBERFsT3ZERDaZH3Io4VFvX1oOdlesN8k47ei8whjHW6a0DoBmpWXuHR9FVbKwP4RrcM2Y4aJMRqRJd0b292t2ZqFaG92Xuyfvp5L01zy4s1To7Tv88fHJZz1wEA2GcsHFs4tLBmNi+WfnjSGxSPebDqeY8dIG1bvXvvbcPeh1/tvb3mZ+nfo7096NfOrnrOcyO9S+HtUlRlUUAL5InS4IulkX/2PmQXEGypN6tZceZrVS+Z8oR3Pr1vRQT/sH3ffd9FgoDHbbUAK4JnLKi+dqr3e3PxV1Vh9eMbpIUfVg/VNgphV3ocJ/3hde9t+/x7rCfZI924xNvrbD67Wfrp2aqv16Y9Jad5e52TKve2GoSbJmBTHTKk1La/HVmB36CQGoA9GjZtRdVsu/3YXpq9NseF77nrtumANqk6rFtzFyTrY292bdZPf/ey4frTKz+7Yfdnv/ijnvrjQI3qWfk/PAAA6htXsC2g99PWRD/xX7993mkveYepW/DOCdzWeHtBdyZwDrF9hlu5YYdlSJMqg6QVjLMgGljnouqNvOvL22asZ9zHVnRY8a2UsMOc+V+/lpZ/sxvfCFWvc2Ftt6XijIVr3/fIhuvbKIHfvUhgBcqsMFlc1bKKxt7HampYcUp7zMeK5NnFBAvXNv+fOfb7HT3dALAPFZaU6YrXZ7h573YR44FTDtAZBwYMdQMAALUfWm0B2VVdD9yi9m5IuMUf60W2gO0vPFm5t2Bq66UbWyFiyZfe24HV5+e9570gYG1xS7Ht2L4dj8d4C+C1HVwVtJd87g3OnUd5Vx0wW1d5g7cvULv9DrcJzCGF4eU1IHQDqGul5RW6+b05en/GOnf/pqO76/JDO7OkGAAAQARheDkABElcTLQeOb2fmjVM0HPfLteDny/W5rxiN2Q/OjoqLOb2z1mbo8lLtmj5lnydNKC1Du++h2tXAwAA1HOEbgCoA9arfcvYnmrWIEF/+2ShXp6yUln5JXr49H4hOSd+w7btbq11C9rfL9uibdur1nD9YNZ6XXxwR910dI+QbDsAAEAoI3QDQB26+JBOatogQX9+Z7Y+nL3erYn+7LmDXBX4YCoqLdePy7Pc+uIWtpduyq/2eMPEWI3o3FQpCbF6b8Zavfj9Cre03JNnDVCHppVLtAAAAOB3EboBoI7Z8OwmKfG6/LXprhf5rOd/1MsXDnZhfH+x8h2LM/P03ZItbr11C9AlZVWVX23Ue982jTWyWzMd2q2p+rVprNgYb6/2mN4Zuum9Oa6K/XFPfq/7Tu6jE/sHVGEFAABAjSikBgD7yew1ObrwlZ9db3eH9GT9509D1S49uc4+zz7HQr71ZH+3dLMyc6svu9IyNVEjuzZzQdvWXm+cHF/je63P2a5r35ypn1dudffPOLCN7jqht5LjuXYLAADqp7VUL985QjeAYFq+OV/nvTRNa7dudz3dr/5psHq32mH9zz2UU1iiX1Zu1c8rszV1eZbrmQ78DZ8YF62hHdNdyB7Ztam6NG+wWxXVy8or9MTXy/Tk10vd+3ZulqJ//XGgerZstE/aX5/Z/4oLSsqVHBcTFsX2AACACN01IXQDCLZNuUUueC/amOfmdj9/3iAN79x0j4qfTVuR7UL2zyu2uuHjO+qe0VAjuzV1QXtwhzQlxu39+p4//LpF1781y/WcW2G124/rpXOGtqvTJdHsa33jp9WuqFt6g3hddHBHje3T0q2FHs7sf8FfLdykJyYudRdJYqOjlJYS7y7I2NfZrHLvvZ+gppW3bbPnUdgOAIDgIXTXgNANIBTkFpXqkld/cXOr42Oi9diZ/XVs35Y1Pt9+Vf+6uaAyYGdr2sps11u+o05NU1y4HtIxTQd3baqMRol10v6s/GJXHO6bxZvd/aN7t9A/Tu2r1OS4ffYZ9jX/8GuW/jt1lSYszHRLmQVql5asS0Z20umD2uyTiwn7U0WFR18uyHRhe8GG3D1+n9SkOH8o9wV0+5kf1r2ZerVsxNrwAADUIUJ3DQjdAEKFVRC/7s1Z+nz+Rlk2uvuE3jpvWAf/UG4LY76ebBs2nlVQUu311snbq1Ujb8jukKYDO6S5tcH3Z3B8acoK/ePzRSot96h14yQ9cVZ/DWqftlfvm1dUqvemr9V/f1zlLjT4DO2YprMPaq9fN+XrP1NXamuhd1mz9JR4XTC8g84d1n6X89JDgX3Pvpi/Uf+cuNSNdDAp8TE6b3gHXTi8g+y6wpb84sqtxF3c8N0OPGbnwo4XIXbUtXkDV8TvhH6t1Dat7moHAABQX60ldO8coRtAKLHgdMcH8/T6T6vd/RP7t3IF0Gas2urm+AayocT92zZ2AXtwxzQNbNdYDRP3Xc/ynpqzNkdX/2+mVmUVuuHeNxzZTZcf2nm35yYv3pjnwvS4metUWPm1WyA9ZWAbnXNQe3Vv0dD/3MKSMr3zy1q98N1yf49/cnyM/jC4nS46pKO7ABBqYfvTeRv05MRl/mkANrXg/OHtdfHBnVx1+919v5ztpS6Ab7YQ7g/lxVqama9JSzZXq04/uEMTV3H+2ANa7vZnAQCAnSN014DQDSDU2K/hJyYu02NfLfnNWtkHtm/iArYF7QPapCohNjSHUVvv9G3j57k51+bgLk316Jn91Lzhroe3l5ZXuJ7f/0xd5Xr1fazI23nD2uvkAa13eWHBRgR8MneDnv12uRZWDtO24G+9u/93aCf1aNEo6BdVrH1PTlzqXwu9YUKsLhzRQX86uGOd9cxv216qz+dt0PiZ6/Xjiix/Qb24mCgd2q2ZC+Cje2YoKT40zycAAMIBobsGhG4AoWrczLWavGSL6822IePWsxtOhcLsfyfvTF+rOz+Yr+2l5a7o1yNn9Hchb0eZuUWuMNr/pq3WpjzvUmb2tR7VK0PnHtRewzqn79Z8ZPvs75Zu0XOTf9WUZVn+4za3+bJDO7uh6ftzfrOF7Y/nrHdztn1D5O0iyp9GdHTbvpz7XpsidB/NXu8CeOD8cRtFcHSfljppQCtXyC+czjUAAEIBobsGhG4AqFvLNuXpqjdm+ucsW4/zn4/q7ipz/7g8W//9caW+mF9VGM2KgP1xSFudNbSdWqYm7ZPh7s9NXq7P5m5wc6RNv7aNddnITjqqd4s6DZfW8/7RnPV68utlWl4ZthslxuqigzvpghEdXOGzYFqSmafxM9e5EQnrcqoK8VktgOP7tnIB/IDWqRRgAwAg0kL3U089pYceekgbN25Uv3799OSTT2rIkCE7fe4rr7yiCy+8sNqxhIQEFRUV1eqzCN0AsH+KxN33yUJXDM1YkCsuK9eSTO8Qa98843OHdXCVz+ti6auVWwr04vfL3dzv4sr5zR2bpuiSQzrplIGt92nFcwvb42et11PfLNOKLd6wbQH74oM76vwRHdQoBObe7zgnfPrqrS6A2/D3nMqidKZTsxSd2K+1C+Dt01OC2k4AAEJZ2ITut956S+edd56effZZDR06VI8//rjeeecdLV68WM2bN99p6L722mvd4z52RT4jI6NWn0foBoD9x+YV3/TuHOUWlbn7SXExrqK2DSG3yuv7gxUX+88PK/Xq1FVurrOvd/2o3hmuPRb4bdk22yfY7YD7vtsJ9rwanvP9si0ubFshOdMkOU4XH9LJzUkPhUJ3v8cKrn27ZLPGz1qnrxZk+i9QGBvuf+uxPQnfAACEc+i2oD148GD961//cvcrKipcw6+++mr99a9/3Wnovu6665STk1Or9y8uLnabz7p169SrVy9CNwDsJ2u3FurpSb+qS7MGOnVQm6ANsS4oLtNbP6/Ri98t1/pttRsdtTvSUuJdL7otXWaVycORFcSzof8fzFqnKcu2uOH5dnHhwoM76KrDu4TFRQQAAPaXsAjdJSUlSk5O1rvvvquTTjrJf/z88893ofqDDz7Yaei++OKL1bp1axfQBw4cqPvvv1+9e/fe6Wfcdddduvvuu39znNANAPWTVUz/fN5GLc3MU3F5hevp9W+B98srXK9vTY/5bjdJiXPF0WxZs5QwDds1zf++9+MFrkCdb3TAX8Z002mD2lJ0DQAAhUnoXr9+vQvPP/zwg4YNG+Y/ftNNN+nbb7/VTz/99JvXTJ06VUuXLlXfvn21bds2Pfzww5o8ebLmz5+/0y+Unm4AAPaM/YkwceEm3ffpQv9c9T6tG+mO43prSMe0YDcPAICwCN1hd0newnlgQB8+fLh69uyp5557Tvfee+9vnm9F1mzzyc2tWi4FAADUzGqmjO6VoZHdmunVH1a6JdDmrcvVGc9N1bF9W+qWsT3UpklysJsJAEBI2/flYndD06ZNFRMTo8zMzGrH7X6LFi1q9R5xcXEaMGCAli1bVketBACgfrOCcZeM7KRv/nKYzhrSTrai2CdzNmjUI9/qkS8Xq7DEWygPAACEWOiOj4/XoEGDNHHiRP8xm6dt9wN7s3elvLxcc+fOVcuWLeuwpQAAwOZ1P3DKAfrk6kN0UKc0N+fd1iQ//OFJGjdzrVuKDAAAhFDoNjfccINeeOEFvfrqq1q4cKEuv/xyFRQU+NfituXEbrnlFv/z77nnHn355Zdavny5ZsyYoXPOOUerVq1yxdUAAEDds+Xe/nfJQXrm7IFq0yRJmbnFuv6t2TrlmR80c/XWYDcPAICQEvQ53WeeeaY2b96sO+64Qxs3blT//v31+eef+9fdXr16taKjq64NbN26VZdccol7bpMmTVxPuRVis+JoAABg/833HntASx3eo7n+/f0Kt1b5rDU5OvnpH3TygNa6+egeapGauMfvv72kXBtzi7RxW5Eyc73b9tJyRUdFyWqnR1dWUHf3o2wv2SPe29X3Ub7XVN5v1iBBI7o0VVJ8zD78jgAAEKLrdIdqhTkAAFB7m3KL9OAXi/Xu9LXuflJcjC4/rLMuHdlJiXFV4daGoGcXlvjDtAXrzG3e/cbcYv/tbdtL67S91r7DujfT0X1a6IgezVmDHAAQmUuGBQOhGwCAujNnbY7u/miBpq/yDjNv3ThJ/ds29vdab8orUmm5p9bB2HrLMxolqEWjRLcOur3S/nSxv14q/HvJox2PeWp4rkcLN+RpXc52/+fEx0RrRJd0je3T0lVrT0uJr7PvDwAgchC6a0DoBgCgbtmfFh/N2aAHPl2oDduKaizK1iLVG6YzGiV696nevTdoJ6pRYqwbGl4X7Zu/Plefzdugz+Zt1PLN3jXITUx0lIZ2THM94GN6t3DtAABgZwjdNSB0AwCwf9i87PdnrlVRaUVlmE5wIbZ5w0S3DFmoWJqZp8/nbXQBfMGG3GqPDWzX2PWAWwhvm8aa5ACAKoTuGhC6AQBATVZnFerz+RtcCJ+xOqfaY71bNdLRvVu4AN41o2Gt37O8wuPWMi8sKVdBsXfvbpeUuQsTNuR9SMc0dzEiEmzYtl1fL9qkyUs2a2thqeJiohQTHa246CjFxkQpNjq6al95LC4m2o0ycLejvbftdbEx3uc0b5TolqmLlO9RKLFz0i42zVm7TYs35qpny0Y656D27mcCYNcI3TUgdAMAgNqwOehfLtioz+Zu1E8rstzccZ/OzVI0vHNTlVUG6oJiC9K+QO29b9XWLdDYeua/x0bRD2jbWEf1bqEje2Woc7MGChd2UcGWirOgbduijXl19lndMhq4yvMjOjfV0E5pFMDbTUWl5S5gz127zYXsuetytGxTfrVz23Rt3kD3ntRHB3VKD1ZTgbBA6K4BoRsAAOyu7IISTViw0fWAf79sS62LwQWyZc1S4mOVnBDj9rZkme2tx9vmmAeyUH9krxY6qneG+rdp7F8iLVRsLSjR5KWbXcj+dslm5RRWVZu3pg5o10SHd2+mjk0bqKyiQmXlHre375uF9NLyCnfBwnfbu/eorPJ41Wu8j1swtLAY+Fer9Yb3bZOqg7s0dRdABrZvrIRYloHzKS4r1+KNed5wbSF73TYtycxz3+sd2fSPA9qkqlPTFL0zfa07380pA1rrlmN6qlnDhCB8BdgVi3DLtxRo9pocV6yyUxhdqIskhO4aELoBAMDeyC0q1dcLN2lxZp6rsJ5s4Tkh1u2T42OVEh/jDdQBx2yfEBtdY2E461WfsDBTX87fqB+XZ1UL9RZ4RvfMcAF8eOf0oARLT2XV928We3uzrWc7MLulJsXp0G7N3PJrI7s1q5MK8BYEp/6apSm/btEPy7ZoZVZhtccT46I1uEOavye8V6tGLpjXB3ZhwgK1L1zbftHG3J1eHGraIF592zTWAa1T3UUL29vwfZ+cwhI99MVivTFttbvI0TAxVn8Z011nD21fb76focoudtn5/92SLe7in28VBquRce+JvXXm4HbBbmK9s5bQvXOEbgAAEOqhftLizZqwIFPfLNqk/OIy/2MW6A/r3twNQT+8e3OlJtfd8GobJj9lWZYL2ZMWb/pNJfoeLRrq8B7NXdC2ofE2/3p/Wru1UD8s84Zwa+eW/OJqj9uFgGGd0jWiq4XwdHVsmlIn1fCDvUTf6z+u1kdz1rupDTtqkhynA9o0Vt/Wqa4n20K29WrX5vtgPai3jZ+nueu2uft9WjfSvSf2caMYsP9GK8xYlaPvlm52Idt+FoHJzZY7bNMkyfV4m9MHtXHTAhLjGPGxvxC6a0DoBgAA4fRH94/Ls93QdgvhmblVwdIKjNmcWwvgtrVqnFTj+1RUeFRUVu4Kt9lc8x33Nte3sPJ27vYyTV2e5XrcSwLmo1tPsvUgW9C2zdZgDxX25+zSTfmasswC+Bb3PQu8WGFapia63vjzhnVwveDhyuoEfDh7vV7/aZXmraualmA90t6e68b+HmwLZHtzocGGoluP94OfL1JeUZmrPfCHwe1089Hd1TiZ9ezr6jz+bukWfb90szuP7d9koO4ZDXVw16Y6pGtTV4AxMTZGz07+VQ9/sdiNPunVspGeOWeg2qenBO3rqE/WErp3jtANAADCkQVn6+n6sjKAL8nMr/a4VZ22nnAXpi1IV4ZoC9O1Kea2MxbajqgM2dZrHC49aDY33IZZ2zB06yG03sKS8qrvwYgu6br4kE46rFuzsOn9XrA+V29MW6XxM9f7LyhYT+cxB7TQH4e214Htm9TZ3P/NecV64LOFen/GOnffpg/8dWwPnTawTcjVGwg3NkLDLhRNdkPGN1e7sGaaNkjQwV3SdUjXZi5s27KLO2Pn+tX/m6msghJ3AeaxM/prdK+M/fRV1F9rCd07R+gGAACRYMWWAn8P+C+rtlYbdrorNrfc5pzbfHT/vvK2hWqbf269ZRa2uzRvEDahdFesR//nldmuSNinczf4i4lZle6LD+moE/u3DskLCtbuj+esd73NMwOWsLOh8n8c0k6nDmpTJ/Pna/LT8izd/sE8/wWfQe2b6G8n9XEXfLBrFrksEK/cUuCGgy/NzHPTIqxA4I7/Pq0H23qyLWjbNI7a/hu02hBXvD7dv9zhFYd11g1HdtvvUz/qk7WE7p0jdAMAgEjsLft5Rba7vWOg9gVpF6pjY+p9z6TNBX9lykq9+fMaf4+xFRezYee2PvX+DLE1sUD2+k+r9f6Mtcot8rbR1i23JeXOHtJOwzqnB+1iiBVte3nKCj3+1VI3isKKq10wvIOuG92VJdwqazJYsF6xk82G6O+MXeTyhewDOzTZqwtANiXERiW8PGWlu2/FF584a4DrMce+R+iuAaEbAAAAFo7emrbGBcj1lUXibN76qQPb6KKDO+73JZhsXr0tSffGT6s1baX3Aoppm5aks4a00+mD2obU0l0btm3XvR8v0KdzN7r7zRsm6Lbjeun4vi0jYnTE7/2sVmYVaMXmAq3w7bcUuGNb8r3Lre2MfVtapSa5kQodmia7avu23F1d/Fw/mr1eN783x10YseJ5T509QIPap2l/VFi3pf5C6VytS4TuGhC6AQAAENhza0POX/xuhb9St4WjUT0y3NDzoR3T6jRELt+cr/9NW613p6/V1sr1zq33eHTP5m6ZLluHPJRHJ9g67Xd+MM+/hJvNl7/nxD7qHEHrRlslfytuZlM5bNk631JdNbFe5U6VwdrWqreQbVv79OT9Oo1h2aY8/d9/p+vXzQWu8OKtx/Z0oxL29flsBR9tGUU7hyct2awKj0djerXQpYd20sAIr3a/ltC9c4RuAAAA7Mj+JP5pRbZe/G65vlq4yX/cKoFbz/cxB7RU3B7OjbX3ziksdWHNtvW237pd89ZvcxWqfVqlJuoPQ9rpzMFtayyYFao9v899u1xPTVrmhjfbUPixfVq68GlLtzVKivXuE+PcMnf+20lxbnRBKPaMW/G4iQsz9dXCTBe4dyxG2CgxVh2bNfCG6/QUdWyWoo7p3qAdSsPsbQrFX9+bo4/nbHD3j+vbUv84ta9SEmL36n3tnLbq+e9OX6MPZq935/fODOmQpktHdnI1IkL54tGeInTXgNANAACAXfl1c77+/f0KvTd9rT9s2RJp1kv4hyFtfxOqrLfcilgFBur12yxgF2nd1kKtzyn6zdJPPpZDbM31sw9qp0O7NXe93OFqVVaB7vpwvr5ZvLnWr7GA7g3mVUHcbqcmxbr7Nmy9W4uG6tGiUZ3Ot7dIZD/3Lxdk6qsFmZq5JqdacUKr5G9L89kICFtyztZAD8WLBTV9ba/8sFL3fbLQDf22AonPnjNQXZo33O332pRXpA9mrne92osz8/zHbQj7KQNbu+J+ttLC85OXa/ysdSot934T7TMvPaSTThzQSgmxoVe0cE8RumtA6AYAAEBtZOUX67UfV+s/U1e6ytOmQUKsW6Zre2mFP1Bn5hXVqnq8zXO19dRbN050Ib5Nk2S3rFMorXm+tyxa2DJt1gu6bXup22z+fK7vtv9Ymb+KfG3Z98+qedta1d0rg3jXjAZ7PGTbPn/6qq1uFQAb3WDzsgPZKIcje2boyN4Z7jPDJWTXZPqqbF3x+gy3LJkVV3zwtL46rm+r3R4+7vu5xcdGa0zvFjp9UBuN6NL0NxeM7ELUyz+s0Bs/rlZeZdFCu4hy4YiO+uPQdu4CS7gjdNeA0A0AAIDdHT49fuY6vfj9Ci3bVH19dB8LIBaeW1UG6laVW5vKfYvUxJBclixYLIIUlJRXC+LVQnllMF+7dbsWZ+ZqTfbO51FbzrPh3RbCvUHc9o3ULi15p6MGbH62rYlt87O/XpTpn0fvW/d8eJd0je6Z4Tb7mUXiSgdXvzFTU5dnufsXjuigW8b2dOdvbYePD2jXWKcNauMCe22Cc25Rqd6cttqNHvGtQ24Xryx42+e3TA3fi06E7hoQugEAALAnbNjspCWb3NzvZg0SqoVrW3Ys3HtCQ5nNTV6SmafFGwO2zDxlV45A2JHNFe+WUdUrbhc8vl60yfXC27xzHwuNo3o0dyMORnZr5sJgpCsrr9CjE5bo6Um/+tdbf+qPA91FhtoMH9/TInklZRX6cPZ6PT/5V/9a71bg7cT+rd28b/s5hRtCdw0I3QAAAED4sxizOb/YH8IXVe4tnO9Y+CyQ9YLb/GzrzR7coYli97BAXriz3v4b3p7l1g9PT4lXv7aNXTX62g4f39uLV899u9xdwPI5vHszXTqysw7qVLcrBuxLhO4aELoBAACAyGWh0Yq6BQZxG7J+cNemLmh3y2gQNqGurtn36bLXZmjhhtw9Hj6+N2atyXE937ZGvW+Kf782qS58H92nRcgXFiR014DQDQAAAABVNQv+9fUyd/ukAa1dpfH9beWWAr34/XK980vVigG2rvndJ/TWYd2bK9yzZeRPWgAAAAAA7JTNd//zmO5BbUOHpin620kH6LrR3fSfqavcigGrsgojZo59ZHwVAAAAAICw1rRBgm44spsuO7STW8btwA5pigT1s2oAAAAAACAkJcfH6oR+v7+GeLggdAMAAAAAUEcI3QAAAAAA1BFCNwAAAAAAdYTQDQAAAABAHSF0AwAAAABQRwjdAAAAAADUEUI3AAAAAAB1hNANAAAAAEAdIXQDAAAAAFBHCN0AAAAAANQRQjcAAAAAAHWE0A0AAAAAQB0hdAMAAAAAUEcI3QAAAAAA1JFY1TMVFRVuv2HDhmA3BQAAAAAQpnyZ0pcxa1LvQndmZqbbDxkyJNhNAQAAAABEQMZs165djY9HeTwej+qRsrIyzZw5UxkZGYqODt3R9Xl5eerVq5cWLFighg0bBrs5AIB6hv8PAQCCJS9M/h9kPdwWuAcMGKDY2Jr7s+td6A4Xubm5Sk1N1bZt29SoUaNgNwcAUM/w/yEAQLDkRtj/g0K3qxcAAAAAgDBH6AYAAAAAoI4QukNUQkKC7rzzTrcHAGB/4/9DAIBgSYiw/wcxpxsAAAAAgDpCTzcAAAAAAHWE0A0AAAAAQB0hdAMAAAAAUEcI3QAAAAAA1BFCd4h66qmn1KFDByUmJmro0KGaNm1asJsEAKgHJk+erOOPP16tWrVSVFSUxo8fH+wmAQDqiQceeECDBw9Ww4YN1bx5c5100klavHixwh2hOwS99dZbuuGGG1yZ/BkzZqhfv34aM2aMNm3aFOymAQAiXEFBgfv/jl38BQBgf/r222915ZVX6scff9SECRNUWlqqo446yv2/KZyxZFgIsp5tu8Lzr3/9y92vqKhQ27ZtdfXVV+uvf/1rsJsHAKgnrKd73LhxrqcBAID9bfPmza7H28L4yJEjFa7o6Q4xJSUlmj59ukaPHu0/Fh0d7e5PnTo1qG0DAAAAgP1l27Ztbp+WlqZwRugOMVu2bFF5ebkyMjKqHbf7GzduDFq7AAAAAGB/qaio0HXXXacRI0aoT58+CmexwW4AAAAAAACBbG73vHnz9P333yvcEbpDTNOmTRUTE6PMzMxqx+1+ixYtgtYuAAAAANgfrrrqKn388cduRY02bdoo3DG8PMTEx8dr0KBBmjhxYrWhFXZ/2LBhQW0bAAAAANQVj8fjArcV8fz666/VsWNHRQJ6ukOQLRd2/vnn68ADD9SQIUP0+OOPuzL5F154YbCbBgCIcPn5+Vq2bJn//ooVKzRr1ixXxKZdu3ZBbRsAIPKHlL/xxhv64IMP3FrdvppWqampSkpKUrhiybAQZcuFPfTQQ+5E69+/v5544gm3lBgAAHVp0qRJOvzww39z3C4Gv/LKK0FpEwCg/ixVuTMvv/yyLrjgAoUrQjcAAAAAAHWEOd0AAAAAANQRQjcAAAAAAHWE0A0AAAAAQB0hdAMAAAAAUEcI3QAAAAAA1BFCNwAAAAAAdYTQDQAAAABAHSF0AwAAAABQRwjdAABgj0VFRWn8+PHBbgYAACGL0A0AQJi64IILXOjdcTv66KOD3TQAAFAp1ncDAACEHwvYL7/8crVjCQkJQWsPAACojp5uAADCmAXsFi1aVNuaNGniHrNe72eeeUZjx45VUlKSOnXqpHfffbfa6+fOnasjjjjCPZ6enq5LL71U+fn51Z7z0ksvqXfv3u6zWrZsqauuuqra41u2bNHJJ5+s5ORkde3aVR9++KH/sa1bt+rss89Ws2bN3GfY4zteJAAAIJIRugEAiGC33367Tj31VM2ePduF3z/84Q9auHChe6ygoEBjxoxxIf3nn3/WO++8o6+++qpaqLbQfuWVV7owbgHdAnWXLl2qfcbdd9+tM844Q3PmzNExxxzjPic7O9v/+QsWLNBnn33mPtfer2nTpvv5uwAAQPBEeTweTxA/HwAA7MWc7tdee02JiYnVjv+///f/3GY93ZdddpkLuj4HHXSQBg4cqKefflovvPCCbr75Zq1Zs0YpKSnu8U8//VTHH3+81q9fr4yMDLVu3VoXXnih/va3v+20DfYZt912m+69915/kG/QoIEL2Tb0/YQTTnAh23rLAQCoj5jTDQBAGDv88MOrhWqTlpbmvz1s2LBqj9n9WbNmudvW89yvXz9/4DYjRoxQRUWFFi9e7AK1he9Ro0btsg19+/b137b3atSokTZt2uTuX3755a6nfcaMGTrqqKN00kknafjw4Xv5VQMAED4I3QAAhDELuTsO995XbA52bcTFxVW7b2Hdgrux+eSrVq1yPegTJkxwAd6Gqz/88MN10mYAAEINc7oBAIhgP/7442/u9+zZ0922vc31tiHhPlOmTFF0dLS6d++uhg0bqkOHDpo4ceJetcGKqJ1//vluKPzjjz+u559/fq/eDwCAcEJPNwAAYay4uFgbN26sdiw2NtZfrMyKox144IE6+OCD9frrr2vatGn697//7R6zgmd33nmnC8R33XWXNm/erKuvvlrnnnuum89t7LjNC2/evLnrtc7Ly3PB3J5XG3fccYcGDRrkqp9bWz/++GN/6AcAoD4gdAMAEMY+//xzt4xXIOulXrRokb+y+JtvvqkrrrjCPe9///ufevXq5R6zJb6++OILXXvttRo8eLC7b/OvH330Uf97WSAvKirSY489pj//+c8uzJ922mm1bl98fLxuueUWrVy50g1XP+SQQ1x7AACoL6heDgBAhLK51ePGjXPFywAAQHAwpxsAAAAAgDpC6AYAAAAAoI4wpxsAgAjFDDIAAIKPnm4AAAAAAOoIoRsAAAAAgDpC6AYAAAAAoI4QugEAAAAAqCOEbgAAAAAA6gihGwAAAACAOkLoBgAAAACgjhC6AQAAAABQ3fj/3LUPHTSuU1IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Chapter5.plot_utils import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting and saving responses\n",
    "\n",
    "While the loss curve gives us some confidence that the fine-tuned model has improved compared to the pre-trained model, we need to look at the response quality to udnerstand how well it performs. In order to do that we will evaluate our model on the hold-out test set.\n",
    "\n",
    "First, we extract the responses for the test samples using our model and then we can evaluate them manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instructions:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct Response:\n",
      "The car is as fast as lightning.\n",
      "\n",
      "Generated Response:\n",
      "The car is as fast as a bullet.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instructions:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct Response:\n",
      "The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Generated Response:\n",
      "A thunderstorm is a type of cloud that typically forms when thunderstorms are strong and can reach altitudes of more than 15,000 feet.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instructions:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct Response:\n",
      "Jane Austen.\n",
      "\n",
      "Generated Response:\n",
      "The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"\\n\\n### Response:\\n\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"Input:\\n{input_text}\\n\")\n",
    "    print(f\"Correct Response:\\n{entry['output']}\\n\")\n",
    "    print(f\"Generated Response:\\n{response_text}\\n\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the responses we see that 2/3 are correct. The second one is not answered correctly. The first response also indicates the difficulty of evaluating the responses in an automated way. The generated response does not match the one given as correct response but it is nevertheless a valid one. In practice such models are evaluated in different ways:\n",
    "\n",
    "- show-answer and multiple choice benchmarks which test general knowledge of a model\n",
    "- human preference comparison between multiple LLMs\n",
    "- automated conversational benchmarks where other LLMs are used to score the responses\n",
    "\n",
    "In order to evaluate our model here, we will implement a method which uses another LLM to score the responses. We will run the LLM locally through ollama.\n",
    "\n",
    "We start off by generating all responses from the test set and saving them to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 110/110 [06:24<00:00,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"\\n\\n### Response:\\n\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-responses.json\", \"w\") as f:\n",
    "    json.dump(test_data, f, indent=4)\n",
    "\n",
    "print(test_data[0])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will save the fine-tuned model weights to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to gpt2-medium355M-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "\n",
    "print(f\"Model saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the fine-tuned LLM\n",
    "\n",
    "As mentioned earlier, we will use ollama to evaluate the test responses. In order to do this we need to download ollama itself and the model we want to use - Llama3 in our case.\n",
    "\n",
    "Once ollama is installed, we can start the application using `ollama serve` and pull the model through the command `ollama pull llama3`. To test that we can load and run the model, we can start an interactive ollama chat session through `ollama run llama3`.\n",
    "\n",
    "We will also implement a function to verify that we can query the model as expected before we start evaluating our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running:  True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "\n",
    "def check_if_running(process_name: str) -> bool:\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "if not check_if_running(\"ollama\"):\n",
    "    raise RuntimeError(\"Ollama is not running\")\n",
    "    \n",
    "print(\"Ollama running: \", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the interactive chat session is good for direct access to llama3, we will access the model through ollamas REST API. We will define a function which queries the model and returns the response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "\n",
    "def query_model(\n",
    "    prompt: str,\n",
    "    model: str = \"llama3\",\n",
    "    url: str = \"http://localhost:11434/api/chat\"\n",
    ") -> str:\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0.,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model=model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful, we can see the response printed above.\n",
    "\n",
    "Now let's test the approach on the first 3 samples of our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Score:\n",
      ">> I'd score my own response a 90 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The original instruction asks for a simile, which is a figure of speech that compares two unlike things using \"like\" or \"as.\"\n",
      "* My response, \"The car is as fast as a bullet,\" meets this requirement by comparing the car's speed to something else (a bullet) using \"as.\"\n",
      "* The comparison is also relevant and evocative, conveying the idea that the car is extremely quick.\n",
      "* However, I could have done better by choosing a more unexpected or creative comparison, like \"The car is as fast as a cheetah\" or \"The car is as fast as a speeding comet.\" These comparisons might have added more surprise and interest to the sentence.\n",
      "\n",
      "Overall, while my response is good, it's not exceptional. I'd give myself 90 out of 100 for this one!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> A thunderstorm is a type of cloud that typically forms when thunderstorms are strong and can reach altitudes of more than 15,000 feet.\n",
      "\n",
      "Score:\n",
      ">> I'd score this model response as 20 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response doesn't directly answer the question about what type of cloud is typically associated with thunderstorms.\n",
      "* Instead, it provides a description of thunderstorms themselves, which is not relevant to the original question.\n",
      "* While the response does mention clouds in passing, it doesn't provide any information about the specific type of cloud that's commonly linked with thunderstorms.\n",
      "\n",
      "A good response would have directly answered the question by stating \"cumulonimbus\" as the typical cloud type associated with thunderstorms. This model response falls short of providing a clear and accurate answer to the original question.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> I'd rate my own response as 100!\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The input instruction is clear and specific: \"Name the author of 'Pride and Prejudice'\".\n",
      "* My response directly answers the question by providing the correct name: \"Jane Austen\".\n",
      "* There's no ambiguity or uncertainty in my response, which makes it a perfect match for the input.\n",
      "\n",
      "So, I'd give myself a score of 100!\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}` \"\n",
    "        f\"score the model response `{entry['model_response']}` \"\n",
    "        \"on a scale from 0 to 100, where 100 is the best score.\"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry[\"output\"])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt, model=model))\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that llama3 provides a reasonable evaluation of each response, scoring the wrong answer to question 2 low.\n",
    "\n",
    "In order to use the returned score we want a less elaborate output from Llama 3 though. We will do this by updating the prompt to Llama 3, asking it to only output the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(json_data: list[dict[str, Any]], json_key: str, model: str = \"llama3\") -> list[dict[str, Any]]:\n",
    "    scores = []\n",
    "    for entry in json_data:\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}` \"\n",
    "            f\"score the model response `{entry['model_response']}` \"\n",
    "            \"on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            \"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model=model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score {score}.\")\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 53.04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores) / len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve the response quality we could:\n",
    "\n",
    "- Adjust the hyperparameters during fine-tuning such as learning rate, batch size or number of epochs\n",
    "- Increase the size of the training set and / or diversify the examples\n",
    "- Adjust the prompts and instruction format to better guide the model\n",
    "- Use larger pretrained models which can capture more complex patterns\n",
    "\n",
    "The Llama 3 8B model for example achives an average score of 82.6 using this methodology. We can use LoRA for example to fine-tune a larger model even with limited resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
